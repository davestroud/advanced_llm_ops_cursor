\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Scaling Up LLM Deployments}{167}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:013}{{6}{167}{Scaling Up LLM Deployments}{chapter.6}{}}
\newlabel{ch:scaling}{{6}{167}{Scaling Up LLM Deployments}{chapter.6}{}}
\newlabel{refsegment:014}{{6}{167}{Scaling Up LLM Deployments}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{168}{section.6.1}\protected@file@percent }
\newlabel{sec:ch6-introduction}{{6.1}{168}{Introduction}{section.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}The Scaling Problem in LLMOps}{168}{section.6.2}\protected@file@percent }
\newlabel{sec:scaling-problem}{{6.2}{168}{The Scaling Problem in LLMOps}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Operational Realities Beyond Baseline Constraints}{169}{subsection.6.2.1}\protected@file@percent }
\newlabel{sec:scaling-operational-realities}{{6.2.1}{169}{Operational Realities Beyond Baseline Constraints}{subsection.6.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}The \ishtar  {} Case}{170}{subsection.6.2.2}\protected@file@percent }
\newlabel{sec:scaling-ishtar-case}{{6.2.2}{170}{The \ishtar {} Case}{subsection.6.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Scaling strategy selection determines cost efficiency and responsiveness. \ishtar  {} employs hierarchical routing, hybrid caching, dynamic context trimming, and predictive autoscaling to handle traffic surges while maintaining latency SLOs. These strategies demonstrate how production systems balance throughput, cost, and quality under variable load.}}{170}{table.6.1}\protected@file@percent }
\newlabel{tab:ch06_ishtar_scaling_strategies}{{6.1}{170}{Scaling strategy selection determines cost efficiency and responsiveness. \ishtar {} employs hierarchical routing, hybrid caching, dynamic context trimming, and predictive autoscaling to handle traffic surges while maintaining latency SLOs. These strategies demonstrate how production systems balance throughput, cost, and quality under variable load}{table.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Broader Perspective}{171}{subsection.6.2.3}\protected@file@percent }
\newlabel{sec:scaling-broader-perspective}{{6.2.3}{171}{Broader Perspective}{subsection.6.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Capacity Planning and SLO Budgets}{171}{subsection.6.2.4}\protected@file@percent }
\newlabel{sec:scaling-capacity-planning}{{6.2.4}{171}{Capacity Planning and SLO Budgets}{subsection.6.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Scaling Dimensions}{172}{section.6.3}\protected@file@percent }
\newlabel{sec:scaling-dimensions}{{6.3}{172}{Scaling Dimensions}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}GPU Partitioning and Multi-Tenancy}{172}{subsection.6.3.1}\protected@file@percent }
\newlabel{sec:gpu-partitioning}{{6.3.1}{172}{GPU Partitioning and Multi-Tenancy}{subsection.6.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Vertical Scaling}{172}{subsection.6.3.2}\protected@file@percent }
\newlabel{sec:scaling-vertical}{{6.3.2}{172}{Vertical Scaling}{subsection.6.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2.1}Characteristics}{172}{subsubsection.6.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2.2}When to use}{173}{subsubsection.6.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2.3}Case in \ishtar  {}}{173}{subsubsection.6.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Horizontal Scaling}{174}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:scaling-horizontal}{{6.3.3}{174}{Horizontal Scaling}{subsection.6.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3.1}When to use}{175}{subsubsection.6.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3.2}Case in \ishtar  {}}{175}{subsubsection.6.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Hybrid Scaling}{175}{subsection.6.3.4}\protected@file@percent }
\newlabel{sec:scaling-hybrid}{{6.3.4}{175}{Hybrid Scaling}{subsection.6.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4.1}When to use}{176}{subsubsection.6.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4.2}Case in \ishtar  {}}{176}{subsubsection.6.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4.3}Lessons Learned}{176}{subsubsection.6.3.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Scaling approach selection balances cost, latency, and complexity. Vertical scaling upgrades hardware but hits single-node limits; horizontal scaling adds capacity but requires load balancing; hybrid scaling combines tiers for cost efficiency. Choose based on traffic patterns, latency requirements, and operational complexity tolerance.}}{177}{table.6.2}\protected@file@percent }
\newlabel{tab:ch06_scaling_matrix}{{6.2}{177}{Scaling approach selection balances cost, latency, and complexity. Vertical scaling upgrades hardware but hits single-node limits; horizontal scaling adds capacity but requires load balancing; hybrid scaling combines tiers for cost efficiency. Choose based on traffic patterns, latency requirements, and operational complexity tolerance}{table.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Distributed Inference Techniques}{177}{section.6.4}\protected@file@percent }
\newlabel{sec:distributed-inference}{{6.4}{177}{Distributed Inference Techniques}{section.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Serving Runtimes and Kernel Optimizations}{177}{subsection.6.4.1}\protected@file@percent }
\newlabel{sec:serving-runtimes-scaling}{{6.4.1}{177}{Serving Runtimes and Kernel Optimizations}{subsection.6.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Scaling dimension selection determines cost efficiency and operational complexity. Vertical scaling upgrades a single node but hits hardware limits; horizontal scaling adds nodes in parallel but requires load balancing; hybrid scaling combines tiers for performance--cost balance but increases management overhead. Choose based on traffic patterns, budget constraints, and operational capabilities.}}{178}{figure.6.1}\protected@file@percent }
\newlabel{fig:ch06_scaling_dimensions}{{6.1}{178}{Scaling dimension selection determines cost efficiency and operational complexity. Vertical scaling upgrades a single node but hits hardware limits; horizontal scaling adds nodes in parallel but requires load balancing; hybrid scaling combines tiers for performance--cost balance but increases management overhead. Choose based on traffic patterns, budget constraints, and operational capabilities}{figure.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Model Parallelism}{178}{subsection.6.4.2}\protected@file@percent }
\newlabel{sec:scaling-model-parallelism}{{6.4.2}{178}{Model Parallelism}{subsection.6.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Tensor Parallelism}{179}{subsection.6.4.3}\protected@file@percent }
\newlabel{sec:scaling-tensor-parallelism}{{6.4.3}{179}{Tensor Parallelism}{subsection.6.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Pipeline Parallelism}{180}{subsection.6.4.4}\protected@file@percent }
\newlabel{sec:scaling-pipeline-parallelism}{{6.4.4}{180}{Pipeline Parallelism}{subsection.6.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.4.1}Case in \ishtar  {}.}{181}{subsubsection.6.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Distributed inference technique selection determines memory efficiency and communication overhead. Model parallelism partitions layers across GPUs; tensor parallelism splits matrix operations; pipeline parallelism stages computation. Choose based on model size, hardware topology, and latency requirements.}}{181}{table.6.3}\protected@file@percent }
\newlabel{tab:ch06_distributed_inference}{{6.3}{181}{Distributed inference technique selection determines memory efficiency and communication overhead. Model parallelism partitions layers across GPUs; tensor parallelism splits matrix operations; pipeline parallelism stages computation. Choose based on model size, hardware topology, and latency requirements}{table.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Distributed inference methods enable large-model deployment but introduce communication overhead. \textbf  {(a)} Model Parallelism partitions layers across GPUs, minimizing inter-GPU communication but requiring high memory per GPU. \textbf  {(b)} Tensor Parallelism splits matrix operations within layers, requiring frequent all-reduce synchronization. \textbf  {(c)} Pipeline Parallelism stages computation, enabling high throughput but adding pipeline bubbles. Choose based on model size, hardware topology, and latency tolerance.}}{182}{figure.6.2}\protected@file@percent }
\newlabel{fig:ch06_parallelism}{{6.2}{182}{Distributed inference methods enable large-model deployment but introduce communication overhead. \textbf {(a)} Model Parallelism partitions layers across GPUs, minimizing inter-GPU communication but requiring high memory per GPU. \textbf {(b)} Tensor Parallelism splits matrix operations within layers, requiring frequent all-reduce synchronization. \textbf {(c)} Pipeline Parallelism stages computation, enabling high throughput but adding pipeline bubbles. Choose based on model size, hardware topology, and latency tolerance}{figure.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Speculative Decoding}{182}{subsection.6.4.5}\protected@file@percent }
\newlabel{sec:scaling-speculative-decoding}{{6.4.5}{182}{Speculative Decoding}{subsection.6.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.1}Principle}{182}{subsubsection.6.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.2}Baseline Algorithm}{182}{subsubsection.6.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.3}Acceptance Intuition}{182}{subsubsection.6.4.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.4}Design Knobs}{182}{subsubsection.6.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.5}Case in \ishtar  {}.}{183}{subsubsection.6.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.5.6}Summary}{183}{subsubsection.6.4.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Batching and Throughput Optimization}{183}{section.6.5}\protected@file@percent }
\newlabel{sec:batching-throughput}{{6.5}{183}{Batching and Throughput Optimization}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Latency decomposition}{183}{subsection.6.5.1}\protected@file@percent }
\newlabel{sec:latency-decomposition}{{6.5.1}{183}{Latency decomposition}{subsection.6.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Static vs.\ dynamic batching}{184}{subsection.6.5.2}\protected@file@percent }
\newlabel{sec:static-dynamic-batching}{{6.5.2}{184}{Static vs.\ dynamic batching}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.1}Choosing the batching window}{184}{subsubsection.6.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Scheduling policies and head-of-line blocking}{184}{subsection.6.5.3}\protected@file@percent }
\newlabel{sec:scheduling-hol}{{6.5.3}{184}{Scheduling policies and head-of-line blocking}{subsection.6.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.4}Continuous batching and preemption}{185}{subsection.6.5.4}\protected@file@percent }
\newlabel{sec:continuous-batching}{{6.5.4}{185}{Continuous batching and preemption}{subsection.6.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.5}Heterogeneous batching and length-aware scheduling}{185}{subsection.6.5.5}\protected@file@percent }
\newlabel{sec:hetero-batching}{{6.5.5}{185}{Heterogeneous batching and length-aware scheduling}{subsection.6.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.6}KV-cache management}{185}{subsection.6.5.6}\protected@file@percent }
\newlabel{sec:kv-cache-mgmt}{{6.5.6}{185}{KV-cache management}{subsection.6.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.6.1}Emerging approaches}{185}{subsubsection.6.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.7}Other throughput levers}{185}{subsection.6.5.7}\protected@file@percent }
\newlabel{sec:throughput-levers}{{6.5.7}{185}{Other throughput levers}{subsection.6.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.7.1}Case in \ishtar  {}}{186}{subsubsection.6.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Autoscaling Strategies}{186}{section.6.6}\protected@file@percent }
\newlabel{sec:autoscaling}{{6.6}{186}{Autoscaling Strategies}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Metrics-Based Autoscaling}{186}{subsection.6.6.1}\protected@file@percent }
\newlabel{sec:scaling-metrics-autoscaling}{{6.6.1}{186}{Metrics-Based Autoscaling}{subsection.6.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.1}Kubernetes scaling primitives}{186}{subsubsection.6.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.2}Control signals}{187}{subsubsection.6.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.3}Replica target computation}{187}{subsubsection.6.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.4}Trigger guards}{187}{subsubsection.6.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.5}Predictive pre-warm (optional but recommended)}{187}{subsubsection.6.6.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.6}Cost-aware placement}{188}{subsubsection.6.6.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1.7}Case in \ishtar  {}}{188}{subsubsection.6.6.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Event-Based Autoscaling}{188}{subsection.6.6.2}\protected@file@percent }
\newlabel{sec:scaling-event-autoscaling}{{6.6.2}{188}{Event-Based Autoscaling}{subsection.6.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2.1}Principle}{188}{subsubsection.6.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2.2}Design components}{189}{subsubsection.6.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2.3}Best practices}{189}{subsubsection.6.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2.4}Case in \ishtar  {}}{189}{subsubsection.6.6.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Metrics-based autoscaling reduces over-provisioning while maintaining SLOs. The autoscaler tracks demand $D(t)$, maintaining capacity above $D(t)$ using headroom $\gamma $ by adjusting replicas $N(t)$. This reactive approach works well for steady traffic but may lag behind sudden spikes, making it suitable for predictable workloads with moderate variability.}}{190}{figure.6.3}\protected@file@percent }
\newlabel{fig:ch06_autoscale_target_tracking}{{6.3}{190}{Metrics-based autoscaling reduces over-provisioning while maintaining SLOs. The autoscaler tracks demand $D(t)$, maintaining capacity above $D(t)$ using headroom $\gamma $ by adjusting replicas $N(t)$. This reactive approach works well for steady traffic but may lag behind sudden spikes, making it suitable for predictable workloads with moderate variability}{figure.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Caching for Scale}{190}{section.6.7}\protected@file@percent }
\newlabel{sec:caching}{{6.7}{190}{Caching for Scale}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Event-based autoscaling eliminates cold-start penalties for predictable traffic patterns. Unlike reactive metrics-based scaling (dotted reference line), event-based scaling provisions capacity \emph  {before} demand spikes arrive, leveraging calendar events or scheduled triggers. This approach pays off when cold-start latency exceeds 30 seconds or when traffic spikes are predictable (daily peaks, product launches, news events).}}{191}{figure.6.4}\protected@file@percent }
\newlabel{fig:ch06_autoscale_event_based}{{6.4}{191}{Event-based autoscaling eliminates cold-start penalties for predictable traffic patterns. Unlike reactive metrics-based scaling (dotted reference line), event-based scaling provisions capacity \emph {before} demand spikes arrive, leveraging calendar events or scheduled triggers. This approach pays off when cold-start latency exceeds 30 seconds or when traffic spikes are predictable (daily peaks, product launches, news events)}{figure.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Response Caching}{191}{subsection.6.7.1}\protected@file@percent }
\newlabel{sec:scaling-response-caching}{{6.7.1}{191}{Response Caching}{subsection.6.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.1.1}Key considerations}{191}{subsubsection.6.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.1.2}Optimizations}{191}{subsubsection.6.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.1.3}Notes and context}{192}{subsubsection.6.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.1.4}Case in \ishtar  {}}{192}{subsubsection.6.7.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Embedding Caching}{192}{subsection.6.7.2}\protected@file@percent }
\newlabel{sec:scaling-embedding-caching}{{6.7.2}{192}{Embedding Caching}{subsection.6.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.2.1}Key considerations}{192}{subsubsection.6.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.2.2}Optimizations}{192}{subsubsection.6.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.2.3}Notes and context}{193}{subsubsection.6.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.2.4}Case in \ishtar  {}}{193}{subsubsection.6.7.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Caching strategies reduce latency and cost for repeated content. \textbf  {(a)} Response caching short-circuits identical requests, eliminating inference cost; keys include query, model version, and decoding parameters. \textbf  {(b)} Embedding caching avoids redundant encoder passes for repeated chunks, reducing RAG latency; keys hash content + embedding model identity. Both strategies are essential for cost-effective RAG systems with recurring queries or document reuse.}}{193}{figure.6.5}\protected@file@percent }
\newlabel{fig:ch06_caching_flow}{{6.5}{193}{Caching strategies reduce latency and cost for repeated content. \textbf {(a)} Response caching short-circuits identical requests, eliminating inference cost; keys include query, model version, and decoding parameters. \textbf {(b)} Embedding caching avoids redundant encoder passes for repeated chunks, reducing RAG latency; keys hash content + embedding model identity. Both strategies are essential for cost-effective RAG systems with recurring queries or document reuse}{figure.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Cost-Aware Scaling}{194}{section.6.8}\protected@file@percent }
\newlabel{sec:cost-aware-scaling}{{6.8}{194}{Cost-Aware Scaling}{section.6.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.1}Mix instance types}{194}{subsubsection.6.8.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.2}Spot/preemptible capacity}{194}{subsubsection.6.8.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.3}Autoscale down aggressively}{194}{subsubsection.6.8.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.4}Batching for cost}{195}{subsubsection.6.8.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.5}Multi-model serving and routing}{195}{subsubsection.6.8.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.6}Throughput-oriented R\&D}{195}{subsubsection.6.8.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.7}Case in \ishtar  {}}{195}{subsubsection.6.8.0.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.0.8}Automated cost planners}{195}{subsubsection.6.8.0.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Scaling Retrieval-Augmented Generation (RAG)}{196}{section.6.9}\protected@file@percent }
\newlabel{sec:scaling-rag}{{6.9}{196}{Scaling Retrieval-Augmented Generation (RAG)}{section.6.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.1}Index sharding}{196}{subsubsection.6.9.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.2}Approximate nearest neighbor (ANN) search}{196}{subsubsection.6.9.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.3}Hot tiers and in-memory caches}{196}{subsubsection.6.9.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.4}Overlap retrieval with generation}{197}{subsubsection.6.9.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.5}Recent directions}{197}{subsubsection.6.9.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.0.6}Case in \ishtar  {}}{197}{subsubsection.6.9.0.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Two-tier RAG scaling optimizes latency and cost for variable query patterns. A router first probes a \emph  {hot} in-memory/GPU tier for low-latency access; on miss, it falls back to a \emph  {distributed} (sharded) ANN index in parallel. Precomputed embedding cache avoids redundant encoding. Results are merged/re-ranked, then fed to the generator. This architecture balances speed (hot tier) with capacity (distributed tier) for production RAG systems.}}{198}{figure.6.6}\protected@file@percent }
\newlabel{fig:ch06_rag_two_tier}{{6.6}{198}{Two-tier RAG scaling optimizes latency and cost for variable query patterns. A router first probes a \emph {hot} in-memory/GPU tier for low-latency access; on miss, it falls back to a \emph {distributed} (sharded) ANN index in parallel. Precomputed embedding cache avoids redundant encoding. Results are merged/re-ranked, then fed to the generator. This architecture balances speed (hot tier) with capacity (distributed tier) for production RAG systems}{figure.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Geographic Scaling}{198}{section.6.10}\protected@file@percent }
\newlabel{sec:geo-scaling}{{6.10}{198}{Geographic Scaling}{section.6.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.1}Latency and compliance benefits}{198}{subsubsection.6.10.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.2}Model placement strategies}{199}{subsubsection.6.10.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.3}Consistency, versioning, and caches}{199}{subsubsection.6.10.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.4}Traffic routing and failover}{199}{subsubsection.6.10.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.5}Data compliance controls}{199}{subsubsection.6.10.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.6}Edge acceleration vs.\ full serving}{199}{subsubsection.6.10.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.7}Decentralized precedent (Petals)}{199}{subsubsection.6.10.0.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.8}Industrial practice}{200}{subsubsection.6.10.0.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.10.0.9}Networking and future directions}{200}{subsubsection.6.10.0.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Geographic scaling reduces latency and enables data residency compliance. Users are routed to the nearest regional cluster (NA/EU/APAC) for low-latency access and regulatory compliance; dashed links illustrate failover/overflow between regions, ensuring availability during regional outages. This architecture is essential for global deployments with strict latency SLOs or data sovereignty requirements.}}{200}{figure.6.7}\protected@file@percent }
\newlabel{fig:ch06_geo_scaling_inset}{{6.7}{200}{Geographic scaling reduces latency and enables data residency compliance. Users are routed to the nearest regional cluster (NA/EU/APAC) for low-latency access and regulatory compliance; dashed links illustrate failover/overflow between regions, ensuring availability during regional outages. This architecture is essential for global deployments with strict latency SLOs or data sovereignty requirements}{figure.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Case Study: Scaling Ishtar AI}{201}{section.6.11}\protected@file@percent }
\newlabel{sec:scaling-ishtar-case-study}{{6.11}{201}{Case Study: Scaling Ishtar AI}{section.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.1}Initial State}{201}{subsection.6.11.1}\protected@file@percent }
\newlabel{sec:scaling-ishtar-initial}{{6.11.1}{201}{Initial State}{subsection.6.11.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.2}Intermediate Stage}{201}{subsection.6.11.2}\protected@file@percent }
\newlabel{sec:scaling-ishtar-intermediate}{{6.11.2}{201}{Intermediate Stage}{subsection.6.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11.3}Mature Stage}{202}{subsection.6.11.3}\protected@file@percent }
\newlabel{sec:scaling-ishtar-mature}{{6.11.3}{202}{Mature Stage}{subsection.6.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces \ishtar  {} scaling timeline demonstrates incremental optimization strategies. The system evolved from a single-GPU prototype to a multi-region, multi-tier deployment with advanced batching, caching, speculative decoding, and predictive autoscaling. This evolution illustrates how production systems grow complexity incrementally, adding optimizations as traffic and requirements increase.}}{202}{figure.6.8}\protected@file@percent }
\newlabel{fig:ch06_ishtar_timeline}{{6.8}{202}{\ishtar {} scaling timeline demonstrates incremental optimization strategies. The system evolved from a single-GPU prototype to a multi-region, multi-tier deployment with advanced batching, caching, speculative decoding, and predictive autoscaling. This evolution illustrates how production systems grow complexity incrementally, adding optimizations as traffic and requirements increase}{figure.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Best Practices Checklist}{203}{section.6.12}\protected@file@percent }
\newlabel{sec:scaling-best-practices}{{6.12}{203}{Best Practices Checklist}{section.6.12}{}}
\@setckpt{ch06-scaling}{
\setcounter{page}{208}
\setcounter{equation}{0}
\setcounter{enumi}{7}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{2}
\setcounter{section}{12}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{3}
\setcounter{chapter}{6}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{1}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{34}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{631}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{14}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{9}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{1}
\setcounter{Item}{50}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
}

\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}LLMOps Fundamentals and Key Concepts}{35}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:04}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{ch:llmops-fundamentals}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{refsegment:05}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is LLMOps?}{36}{section.2.1}\protected@file@percent }
\newlabel{sec:ch2-what-is-llmops}{{2.1}{36}{What is LLMOps?}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definition}{36}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Why LLMOps is Different from MLOps}{36}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Massive scale and structural complexity}{37}{subsubsection.2.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Probabilistic output behavior}{37}{subsubsection.2.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}Finite context window constraints}{38}{subsubsection.2.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.4}Ethical and reliability risks}{38}{subsubsection.2.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.5}Supporting Equations (Capacity, Cost, and Complexity)}{39}{subsubsection.2.1.2.5}\protected@file@percent }
\newlabel{sec:ch2-supporting-equations}{{2.1.2.5}{39}{Supporting Equations (Capacity, Cost, and Complexity)}{subsubsection.2.1.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.6}Parameter memory (inference)}{39}{subsubsection.2.1.2.6}\protected@file@percent }
\newlabel{eq:param-mem}{{2.1}{39}{Parameter memory (inference)}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.7}KV-cache memory (inference)}{39}{subsubsection.2.1.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.8}Serving efficiency and KV-cache management}{39}{subsubsection.2.1.2.8}\protected@file@percent }
\newlabel{eq:kv-cache}{{2.2}{39}{Serving efficiency and KV-cache management}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.9}Activation memory (training/fine-tuning)}{40}{subsubsection.2.1.2.9}\protected@file@percent }
\newlabel{eq:act-mem}{{2.3}{40}{Activation memory (training/fine-tuning)}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.10}Per-layer FLOPs (forward)}{40}{subsubsection.2.1.2.10}\protected@file@percent }
\newlabel{eq:flops-layer}{{2.4}{40}{Per-layer FLOPs (forward)}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.11}Attention complexity}{41}{subsubsection.2.1.2.11}\protected@file@percent }
\newlabel{eq:attn-complexity}{{2.5}{41}{Attention complexity}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.12}Throughput and batching}{41}{subsubsection.2.1.2.12}\protected@file@percent }
\newlabel{eq:latency}{{2.6}{41}{Throughput and batching}{equation.2.6}{}}
\newlabel{eq:throughput}{{2.7}{42}{Throughput and batching}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.13}Temperature and determinism}{42}{subsubsection.2.1.2.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematic of a Transformer block highlighting attention, FFN, and residual paths. Inference memory is dominated by parameters and the KV cache (Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}); compute cost scales with Eq.~\ref {eq:flops-layer}.}}{43}{figure.2.1}\protected@file@percent }
\newlabel{fig:transformer-block}{{2.1}{43}{Schematic of a Transformer block highlighting attention, FFN, and residual paths. Inference memory is dominated by parameters and the KV cache (Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}); compute cost scales with Eq.~\ref {eq:flops-layer}}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention, local attention, and RAG to manage long contexts in production.}}{43}{figure.2.2}\protected@file@percent }
\newlabel{fig:quadratic-attn}{{2.2}{43}{Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention, local attention, and RAG to manage long contexts in production}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost.}}{44}{figure.2.3}\protected@file@percent }
\newlabel{fig:llm-scale}{{2.3}{44}{Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.14}Illustrative Diagrams}{44}{subsubsection.2.1.2.14}\protected@file@percent }
\newlabel{sec:ch2-illustrative-diagrams}{{2.1.2.14}{44}{Illustrative Diagrams}{subsubsection.2.1.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Idealized throughput vs. TTFT using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$. Larger batches ($B$) amortize TTFT, but real systems saturate due to kernel/IO limits; use as a planning guideline, not a guarantee.}}{45}{figure.2.4}\protected@file@percent }
\newlabel{fig:ttft-throughput}{{2.4}{45}{Idealized throughput vs. TTFT using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$. Larger batches ($B$) amortize TTFT, but real systems saturate due to kernel/IO limits; use as a planning guideline, not a guarantee}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Core Components of an LLMOps Pipeline}{45}{section.2.2}\protected@file@percent }
\newlabel{sec:ch2-core-components}{{2.2}{45}{Core Components of an LLMOps Pipeline}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Key Concepts in LLMOps}{47}{section.2.3}\protected@file@percent }
\newlabel{sec:ch2-key-concepts}{{2.3}{47}{Key Concepts in LLMOps}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Prompt Engineering}{47}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Retrieval-Augmented Generation (RAG)}{48}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Tool Calling and Structured Outputs}{49}{subsection.2.3.3}\protected@file@percent }
\newlabel{sec:ch2-tool-calling}{{2.3.3}{49}{Tool Calling and Structured Outputs}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.1}Ops implications}{49}{subsubsection.2.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Evaluation Metrics}{49}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4.1}Evaluation Frameworks and Tooling}{50}{subsubsection.2.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Human Feedback and Alignment}{51}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Security, Privacy, and Threat Modeling}{52}{subsection.2.3.6}\protected@file@percent }
\newlabel{sec:ch2-security}{{2.3.6}{52}{Security, Privacy, and Threat Modeling}{subsection.2.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6.1}Operational controls}{52}{subsubsection.2.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Transformer Architecture Foundations for LLMOps}{52}{subsection.2.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.1}Self-attention and multi-head attention}{52}{subsubsection.2.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.2}Positional encodings}{53}{subsubsection.2.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.3}Feed-forward networks (FFN)}{54}{subsubsection.2.3.7.3}\protected@file@percent }
\newlabel{eq:ffn-formula}{{2.13}{54}{Feed-forward networks (FFN)}{equation.2.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.4}Residual connections and LayerNorm}{55}{subsubsection.2.3.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.5}Worked example (KV-cache sizing)}{56}{subsubsection.2.3.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.7.6}Rule-of-thumb parameter memory}{57}{subsubsection.2.3.7.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Approximate parameter memory requirements for different model scales (FP16 weights).}}{57}{table.2.1}\protected@file@percent }
\newlabel{tab:param-memory}{{2.1}{57}{Approximate parameter memory requirements for different model scales (FP16 weights)}{table.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The LLM Lifecycle}{58}{section.2.4}\protected@file@percent }
\newlabel{sec:ch2-lifecycle}{{2.4}{58}{The LLM Lifecycle}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Governance and Risk Management}{58}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec:ch2-governance}{{2.4.1}{58}{Governance and Risk Management}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.1}LLMOps linkage}{58}{subsubsection.2.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Tools and Frameworks}{60}{section.2.5}\protected@file@percent }
\newlabel{sec:ch2-tools}{{2.5}{60}{Tools and Frameworks}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Ishtar AI: A Running Example}{61}{section.2.6}\protected@file@percent }
\newlabel{sec:ch2-ishtar-running-example}{{2.6}{61}{Ishtar AI: A Running Example}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.0.1}A concrete query trace}{61}{subsubsection.2.6.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.0.2}Release gates for reliability}{61}{subsubsection.2.6.0.2}\protected@file@percent }
\@setckpt{ch02-llmops-fundamentals}{
\setcounter{page}{63}
\setcounter{equation}{26}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{1}
\setcounter{chapter}{2}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{tcbbreakpart}{2}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{3}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{5}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{3}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
}

\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}LLMOps Fundamentals and Key Concepts}{35}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:04}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{ch:llmops-fundamentals}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{refsegment:05}{{2}{35}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is LLMOps?}{36}{section.2.1}\protected@file@percent }
\newlabel{sec:ch2-what-is-llmops}{{2.1}{36}{What is LLMOps?}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definition}{36}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Why LLMOps is Different from MLOps}{36}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Massive scale and structural complexity}{37}{subsubsection.2.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Probabilistic output behavior}{37}{subsubsection.2.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}Finite context window constraints}{38}{subsubsection.2.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.4}Ethical and reliability risks}{38}{subsubsection.2.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.5}Supporting Equations (Capacity, Cost, and Complexity)}{39}{subsubsection.2.1.2.5}\protected@file@percent }
\newlabel{sec:ch2-supporting-equations}{{2.1.2.5}{39}{Supporting Equations (Capacity, Cost, and Complexity)}{subsubsection.2.1.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.6}Parameter memory (inference)}{39}{subsubsection.2.1.2.6}\protected@file@percent }
\newlabel{eq:param-mem}{{2.1}{39}{Parameter memory (inference)}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.7}KV-cache memory (inference)}{39}{subsubsection.2.1.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.8}Serving efficiency and KV-cache management}{39}{subsubsection.2.1.2.8}\protected@file@percent }
\newlabel{eq:kv-cache}{{2.2}{39}{Serving efficiency and KV-cache management}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.9}Activation memory (training/fine-tuning)}{40}{subsubsection.2.1.2.9}\protected@file@percent }
\newlabel{eq:act-mem}{{2.3}{40}{Activation memory (training/fine-tuning)}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.10}Per-layer FLOPs (forward)}{40}{subsubsection.2.1.2.10}\protected@file@percent }
\newlabel{eq:flops-layer}{{2.4}{40}{Per-layer FLOPs (forward)}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.11}Attention complexity}{41}{subsubsection.2.1.2.11}\protected@file@percent }
\newlabel{eq:attn-complexity}{{2.5}{41}{Attention complexity}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.12}Throughput and batching}{41}{subsubsection.2.1.2.12}\protected@file@percent }
\newlabel{eq:latency}{{2.6}{41}{Throughput and batching}{equation.2.6}{}}
\newlabel{eq:throughput}{{2.7}{42}{Throughput and batching}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.13}Temperature and determinism}{42}{subsubsection.2.1.2.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematic of a Transformer block highlighting attention, FFN, and residual paths. Inference memory is dominated by parameters and the KV cache (Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}); compute cost scales with Eq.~\ref {eq:flops-layer}.}}{43}{figure.2.1}\protected@file@percent }
\newlabel{fig:transformer-block}{{2.1}{43}{Schematic of a Transformer block highlighting attention, FFN, and residual paths. Inference memory is dominated by parameters and the KV cache (Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}); compute cost scales with Eq.~\ref {eq:flops-layer}}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention, local attention, and RAG to manage long contexts in production.}}{43}{figure.2.2}\protected@file@percent }
\newlabel{fig:quadratic-attn}{{2.2}{43}{Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention, local attention, and RAG to manage long contexts in production}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost.}}{44}{figure.2.3}\protected@file@percent }
\newlabel{fig:llm-scale}{{2.3}{44}{Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.14}Illustrative Diagrams}{44}{subsubsection.2.1.2.14}\protected@file@percent }
\newlabel{sec:ch2-illustrative-diagrams}{{2.1.2.14}{44}{Illustrative Diagrams}{subsubsection.2.1.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Idealized throughput vs. TTFT using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$. Larger batches ($B$) amortize TTFT, but real systems saturate due to kernel/IO limits; use as a planning guideline, not a guarantee.}}{45}{figure.2.4}\protected@file@percent }
\newlabel{fig:ttft-throughput}{{2.4}{45}{Idealized throughput vs. TTFT using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$. Larger batches ($B$) amortize TTFT, but real systems saturate due to kernel/IO limits; use as a planning guideline, not a guarantee}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Core Components of an LLMOps Pipeline}{45}{section.2.2}\protected@file@percent }
\newlabel{sec:ch2-core-components}{{2.2}{45}{Core Components of an LLMOps Pipeline}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Key Concepts in LLMOps}{47}{section.2.3}\protected@file@percent }
\newlabel{sec:ch2-key-concepts}{{2.3}{47}{Key Concepts in LLMOps}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Prompt Engineering}{47}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Retrieval-Augmented Generation (RAG)}{48}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Tool Calling and Structured Outputs}{49}{subsection.2.3.3}\protected@file@percent }
\newlabel{sec:ch2-tool-calling}{{2.3.3}{49}{Tool Calling and Structured Outputs}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.1}Ops implications}{49}{subsubsection.2.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Evaluation Metrics}{49}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4.1}Evaluation Frameworks and Tooling}{50}{subsubsection.2.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Human Feedback and Alignment}{51}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Security, Privacy, and Threat Modeling}{52}{subsection.2.3.6}\protected@file@percent }
\newlabel{sec:ch2-security}{{2.3.6}{52}{Security, Privacy, and Threat Modeling}{subse
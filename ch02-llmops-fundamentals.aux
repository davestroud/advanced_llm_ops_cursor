\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}LLMOps Fundamentals and Key Concepts}{37}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:04}{{2}{37}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{ch:llmops-fundamentals}{{2}{37}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{refsegment:05}{{2}{37}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is LLMOps?}{38}{section.2.1}\protected@file@percent }
\newlabel{sec:ch2-what-is-llmops}{{2.1}{38}{What is LLMOps?}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definition}{38}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Why LLMOps is Different from MLOps}{38}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Massive scale and structural complexity}{39}{subsubsection.2.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Probabilistic output behavior}{39}{subsubsection.2.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}Finite context window constraints}{40}{subsubsection.2.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.4}Ethical and reliability risks}{40}{subsubsection.2.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.5}Supporting Equations (Capacity, Cost, and Complexity)}{41}{subsubsection.2.1.2.5}\protected@file@percent }
\newlabel{sec:ch2-supporting-equations}{{2.1.2.5}{41}{Supporting Equations (Capacity, Cost, and Complexity)}{subsubsection.2.1.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.6}Parameter memory (inference)}{41}{subsubsection.2.1.2.6}\protected@file@percent }
\newlabel{eq:param-mem}{{2.1}{41}{Parameter memory (inference)}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.7}KV-cache memory (inference)}{41}{subsubsection.2.1.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.8}Serving efficiency and KV-cache management}{41}{subsubsection.2.1.2.8}\protected@file@percent }
\newlabel{eq:kv-cache}{{2.2}{41}{Serving efficiency and KV-cache management}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.9}Activation memory (training/fine-tuning)}{42}{subsubsection.2.1.2.9}\protected@file@percent }
\newlabel{eq:act-mem}{{2.3}{42}{Activation memory (training/fine-tuning)}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.10}Per-layer FLOPs (forward)}{42}{subsubsection.2.1.2.10}\protected@file@percent }
\newlabel{eq:flops-layer}{{2.4}{42}{Per-layer FLOPs (forward)}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.11}Attention complexity}{43}{subsubsection.2.1.2.11}\protected@file@percent }
\newlabel{eq:attn-complexity}{{2.5}{43}{Attention complexity}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.12}Throughput and batching}{43}{subsubsection.2.1.2.12}\protected@file@percent }
\newlabel{eq:latency}{{2.6}{43}{Throughput and batching}{equation.2.6}{}}
\newlabel{eq:throughput}{{2.7}{44}{Throughput and batching}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.13}Temperature and determinism}{44}{subsubsection.2.1.2.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schematic of a Transformer block highlighting attention, FFN, and residual paths. Inference memory is dominated by parameters and the KV cache (Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}); compute cost scales with Eq.~\ref {eq:flops-layer}.}}{45}{figure.2.1}\protected@file@percent }
\newlabel{fig:transformer-block}{{2
\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}LLMOps Fundamentals and Key Concepts}{45}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:05}{{2}{45}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{ch:llmops-fundamentals}{{2}{45}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\newlabel{refsegment:06}{{2}{45}{LLMOps Fundamentals and Key Concepts}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{46}{section.2.1}\protected@file@percent }
\newlabel{sec:ch2-introduction}{{2.1}{46}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}What is LLMOps?}{47}{section.2.2}\protected@file@percent }
\newlabel{sec:ch2-what-is-llmops}{{2.2}{47}{What is LLMOps?}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definition}{47}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Why LLMOps is Different from MLOps}{47}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.1}Massive scale and structural complexity}{47}{subsubsection.2.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.2}Probabilistic output behavior}{48}{subsubsection.2.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.3}Finite context window constraints}{48}{subsubsection.2.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.4}Ethical and reliability risks}{48}{subsubsection.2.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.5}Supporting Equations (Capacity, Cost, and Complexity)}{49}{subsubsection.2.2.2.5}\protected@file@percent }
\newlabel{sec:ch2-supporting-equations}{{2.2.2.5}{49}{Supporting Equations (Capacity, Cost, and Complexity)}{subsubsection.2.2.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.6}Parameter memory (inference)}{49}{subsubsection.2.2.2.6}\protected@file@percent }
\newlabel{eq:param-mem}{{2.1}{49}{Parameter memory (inference)}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.7}KV-cache memory (inference)}{50}{subsubsection.2.2.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.8}Serving efficiency and KV-cache management}{50}{subsubsection.2.2.2.8}\protected@file@percent }
\newlabel{eq:kv-cache}{{2.2}{50}{Serving efficiency and KV-cache management}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.9}Activation memory (training/fine-tuning)}{50}{subsubsection.2.2.2.9}\protected@file@percent }
\newlabel{eq:act-mem}{{2.3}{50}{Activation memory (training/fine-tuning)}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.10}Per-layer FLOPs (forward)}{51}{subsubsection.2.2.2.10}\protected@file@percent }
\newlabel{eq:flops-layer}{{2.4}{51}{Per-layer FLOPs (forward)}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.11}Attention complexity}{51}{subsubsection.2.2.2.11}\protected@file@percent }
\newlabel{eq:attn-complexity}{{2.5}{51}{Attention complexity}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.12}Throughput and batching}{52}{subsubsection.2.2.2.12}\protected@file@percent }
\newlabel{eq:latency}{{2.6}{52}{Throughput and batching}{equation.2.6}{}}
\newlabel{eq:throughput}{{2.7}{52}{Throughput and batching}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.13}Temperature and determinism}{53}{subsubsection.2.2.2.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.14}Illustrative Diagrams}{53}{subsubsection.2.2.2.14}\protected@file@percent }
\newlabel{sec:ch2-illustrative-diagrams}{{2.2.2.14}{53}{Illustrative Diagrams}{subsubsection.2.2.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Transformer architecture determines inference memory and compute requirements. The attention mechanism, FFN layers, and residual paths each contribute to memory (dominated by parameters and KV cache, Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}) and compute cost (scales with Eq.~\ref {eq:flops-layer}). Understanding these components enables capacity planning and optimization decisions.}}{54}{figure.2.1}\protected@file@percent }
\newlabel{fig:ch02_transformer_block}{{2.1}{54}{Transformer architecture\index {Transformer!architecture} determines inference memory and compute requirements. The attention mechanism\index {attention!mechanism}, FFN layers\index {FFN}, and residual paths\index {residual connection} each contribute to memory (dominated by parameters and KV cache, Eq.~\ref {eq:param-mem}, \ref {eq:kv-cache}) and compute cost (scales with Eq.~\ref {eq:flops-layer}). Understanding these components enables capacity planning and optimization decisions}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention, local attention, and RAG to manage long contexts in production.}}{55}{figure.2.2}\protected@file@percent }
\newlabel{fig:ch02_quadratic_attn}{{2.2}{55}{Attention cost grows quadratically with context length $T$ (solid), outpacing linear strategies (dashed). This motivates paged attention\index {attention!paged}, local attention\index {attention!local}, and RAG to manage long contexts in production}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost.}}{55}{figure.2.3}\protected@file@percent }
\newlabel{fig:ch02_llm_scale}{{2.3}{55}{Empirical growth in Transformer-based model size (log scale). Parameter counts have increased by orders of magnitude, driving distinct operational constraints in memory, compute, and cost}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Core Components of an LLMOps Pipeline}{55}{section.2.3}\protected@file@percent }
\newlabel{sec:ch2-core-components}{{2.3}{55}{Core Components of an LLMOps Pipeline}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Throughput vs.\ TTFT trade-off determines batching strategy and capacity planning. Larger batches ($B$) amortize TTFT overhead, improving throughput, but real systems saturate due to kernel/IO limits before reaching theoretical maximums. Understanding this trade-off helps teams optimize batch sizes for their latency requirements. Using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$ provides planning guidelines, not guarantees.}}{56}{figure.2.4}\protected@file@percent }
\newlabel{fig:ch02_ttft_throughput}{{2.4}{56}{Throughput vs.\ TTFT trade-off determines batching strategy and capacity planning. Larger batches ($B$) amortize TTFT overhead, improving throughput, but real systems saturate due to kernel/IO limits before reaching theoretical maximums. Understanding this trade-off helps teams optimize batch sizes for their latency requirements. Using Eq.~\ref {eq:throughput} with $N{=}120$ tokens/request and $\tau _1{=}\SI {0.02}{s/token}$ provides planning guidelines, not guarantees}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Key Concepts in LLMOps}{58}{section.2.4}\protected@file@percent }
\newlabel{sec:ch2-key-concepts}{{2.4}{58}{Key Concepts in LLMOps}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Prompt Engineering}{58}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Retrieval-Augmented Generation (RAG)}{59}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Tool Calling and Structured Outputs}{60}{subsection.2.4.3}\protected@file@percent }
\newlabel{sec:ch2-tool-calling}{{2.4.3}{60}{Tool Calling and Structured Outputs}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3.1}Ops implications}{60}{subsubsection.2.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Evaluation Metrics}{60}{subsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4.1}Evaluation Frameworks and Tooling}{61}{subsubsection.2.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Human Feedback and Alignment}{61}{subsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Security, Privacy, and Threat Modeling}{62}{subsection.2.4.6}\protected@file@percent }
\newlabel{sec:ch2-security}{{2.4.6}{62}{Security, Privacy, and Threat Modeling}{subsection.2.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6.1}Operational controls}{63}{subsubsection.2.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Transformer Architecture Foundations for LLMOps}{63}{subsection.2.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.1}Self-attention and multi-head attention}{63}{subsubsection.2.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.2}Positional encodings}{64}{subsubsection.2.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.3}Feed-forward networks (FFN)}{64}{subsubsection.2.4.7.3}\protected@file@percent }
\newlabel{eq:ffn-formula}{{2.13}{65}{Feed-forward networks (FFN)}{equation.2.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.4}Residual connections and LayerNorm}{65}{subsubsection.2.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.5}Worked example (KV-cache sizing)}{67}{subsubsection.2.4.7.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Parameter memory requirements determine deployment feasibility and hardware selection. Memory scales linearly with parameter count, making large models (70B+) require multi-GPU deployments even with quantization. Understanding these requirements enables teams to plan infrastructure investments and choose appropriate model sizes for their constraints.}}{68}{table.2.1}\protected@file@percent }
\newlabel{tab:ch02_param_memory}{{2.1}{68}{Parameter memory requirements determine deployment feasibility and hardware selection. Memory scales linearly with parameter count, making large models (70B+) require multi-GPU deployments even with quantization. Understanding these requirements enables teams to plan infrastructure investments and choose appropriate model sizes for their constraints}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7.6}Rule-of-thumb parameter memory}{68}{subsubsection.2.4.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The LLM Lifecycle}{69}{section.2.5}\protected@file@percent }
\newlabel{sec:ch2-lifecycle}{{2.5}{69}{The LLM Lifecycle}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Governance and Risk Management}{69}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec:ch2-governance}{{2.5.1}{69}{Governance and Risk Management}{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1.1}LLMOps linkage}{69}{subsubsection.2.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Tools and Frameworks}{70}{section.2.6}\protected@file@percent }
\newlabel{sec:ch2-tools}{{2.6}{70}{Tools and Frameworks}{section.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Ishtar AI: A Running Example}{72}{section.2.7}\protected@file@percent }
\newlabel{sec:ch2-ishtar-running-example}{{2.7}{72}{Ishtar AI: A Running Example}{section.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.0.1}A concrete query trace}{72}{subsubsection.2.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.0.2}Release gates for reliability}{72}{subsubsection.2.7.0.2}\protected@file@percent }
\@setckpt{ch02-llmops-fundamentals}{
\setcounter{page}{75}
\setcounter{equation}{26}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{1}
\setcounter{chapter}{2}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{1}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{2}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{6}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{112}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{6}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{9}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{3}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
}

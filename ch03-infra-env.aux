\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Infrastructure and Environment for LLMOps}{85}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:07}{{3}{85}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\newlabel{ch:infra}{{3}{85}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\newlabel{refsegment:08}{{3}{85}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{86}{section.3.1}\protected@file@percent }
\newlabel{sec:ch3-introduction}{{3.1}{86}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Hardware Selection for LLM Workloads}{86}{section.3.2}\protected@file@percent }
\newlabel{sec:infra-hardware-bench}{{3.2}{86}{Hardware Selection for LLM Workloads}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Compute Profiles and Workload Types}{87}{subsection.3.2.1}\protected@file@percent }
\newlabel{sec:infra-hardware-profiles}{{3.2.1}{87}{Compute Profiles and Workload Types}{subsection.3.2.1}{}}
\newlabel{eq:latency-infra}{{3.1}{87}{Compute Profiles and Workload Types}{equation.3.1}{}}
\newlabel{eq:throughput-env}{{3.2}{87}{Compute Profiles and Workload Types}{equation.3.2}{}}
\newlabel{eq:kv-memory}{{3.3}{87}{Compute Profiles and Workload Types}{equation.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces GPU accelerator selection determines cost, throughput, and deployment feasibility. Different accelerators optimize for different model sizes and workloads: A100 provides balanced performance, H100 offers higher throughput and FP8 support, L4 enables cost-effective smaller models. Choose based on model size, traffic volume, and budget constraints.}}{88}{table.3.1}\protected@file@percent }
\newlabel{tab:ch03_gpucompare}{{3.1}{88}{GPU accelerator selection determines cost, throughput, and deployment feasibility. Different accelerators optimize for different model sizes and workloads: A100 provides balanced performance, H100 offers higher throughput and FP8 support, L4 enables cost-effective smaller models. Choose based on model size, traffic volume, and budget constraints}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}GPU Architectures and Choices}{88}{subsection.3.2.2}\protected@file@percent }
\newlabel{sec:infra-hardware-gpu}{{3.2.2}{88}{GPU Architectures and Choices}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}TPU Architectures and Considerations}{89}{subsection.3.2.3}\protected@file@percent }
\newlabel{sec:infra-hardware-tpu}{{3.2.3}{89}{TPU Architectures and Considerations}{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Cost Modeling and Economics}{89}{section.3.3}\protected@file@percent }
\newlabel{sec:infra-cost}{{3.3}{89}{Cost Modeling and Economics}{section.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Cost per token varies significantly with utilization and hardware choice. Higher utilization amortizes fixed costs, making H100 cost-effective at scale despite higher hourly rates. Choose A100 for variable workloads with low average utilization; choose H100 for sustained high-throughput deployments where utilization exceeds 60\%.}}{90}{table.3.2}\protected@file@percent }
\newlabel{tab:ch03_cost_per_token}{{3.2}{90}{Cost per token varies significantly with utilization and hardware choice. Higher utilization amortizes fixed costs, making H100 cost-effective at scale despite higher hourly rates. Choose A100 for variable workloads with low average utilization; choose H100 for sustained high-throughput deployments where utilization exceeds 60\%}{table.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Token Economics and Cost per Query}{90}{subsection.3.3.1}\protected@file@percent }
\newlabel{sec:infra-cost-tokens}{{3.3.1}{90}{Token Economics and Cost per Query}{subsection.3.3.1}{}}
\newlabel{eq:costptok}{{3.3.1}{90}{Token Economics and Cost per Query}{subsection.3.3.1}{}}
\newlabel{lst:ch03_cost_calculation}{{3.1}{91}{Token Economics and Cost per Query}{llmlisting.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Batch Size vs Throughput Trade-offs}{94}{subsection.3.3.2}\protected@file@percent }
\newlabel{sec:infra-cost-batch}{{3.3.2}{94}{Batch Size vs Throughput Trade-offs}{subsection.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Throughput vs.\ batch size trade-off determines capacity planning and latency SLOs. Increasing batch size initially yields large throughput gains (e.g., 3$\times $ improvement from batch 1 to 4), but returns diminish beyond batch size 32. Eventually throughput saturates (batch 64 and 128 achieve similar tokens/sec) as memory or kernel launch overhead becomes the bottleneck. Understanding this saturation point helps teams optimize batch sizes for their latency requirements and hardware constraints.}}{95}{figure.3.1}\protected@file@percent }
\newlabel{fig:ch03_batchtradeoff}{{3.1}{95}{Throughput vs.\ batch size trade-off determines capacity planning and latency SLOs. Increasing batch size initially yields large throughput gains (e.g., 3$\times $ improvement from batch 1 to 4), but returns diminish beyond batch size 32. Eventually throughput saturates (batch 64 and 128 achieve similar tokens/sec) as memory or kernel launch overhead becomes the bottleneck. Understanding this saturation point helps teams optimize batch sizes for their latency requirements and hardware constraints}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Caching and Quantization Effects}{95}{subsection.3.3.3}\protected@file@percent }
\newlabel{sec:infra-cost-quant}{{3.3.3}{95}{Caching and Quantization Effects}{subsection.3.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.1}KV Cache and Prompt Caching}{95}{subsubsection.3.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.2}Quantization}{96}{subsubsection.3.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.3}Ishtar Case}{96}{subsubsection.3.3.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Cost per token varies significantly across accelerators and utilization levels. Lower-cost accelerators (L4) become cost-effective for smaller models or variable workloads, while high-end accelerators (H100) justify their premium at high utilization with large models. When cost differences exceed 2$\times $, hardware choice becomes a primary cost optimization lever. Assumes typical sustained throughput for a 13B--70B model and on-demand cloud pricing.}}{97}{table.3.3}\protected@file@percent }
\newlabel{tab:ch03_costcompare}{{3.3}{97}{Cost per token varies significantly across accelerators and utilization levels. Lower-cost accelerators (L4) become cost-effective for smaller models or variable workloads, while high-end accelerators (H100) justify their premium at high utilization with large models. When cost differences exceed 2$\times $, hardware choice becomes a primary cost optimization lever. Assumes typical sustained throughput for a 13B--70B model and on-demand cloud pricing}{table.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Worked Example: Cost per Million Tokens Across Accelerators}{97}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Infrastructure-as-Code (IaC) for LLMOps}{98}{section.3.4}\protected@file@percent }
\newlabel{sec:iac}{{3.4}{98}{Infrastructure-as-Code (IaC) for LLMOps}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Why IaC Matters}{98}{subsection.3.4.1}\protected@file@percent }
\newlabel{sec:iac-why}{{3.4.1}{98}{Why IaC Matters}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tooling Comparison}{99}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:iac-tools}{{3.4.2}{99}{Tooling Comparison}{subsection.3.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Infrastructure-as-Code tool selection determines deployment reproducibility and operational overhead. Terraform provides declarative multi-cloud support, Pulumi enables programmatic infrastructure with familiar languages, and Ansible offers agentless configuration management. Choose based on cloud provider mix, team expertise, and complexity requirements.}}{100}{table.3.4}\protected@file@percent }
\newlabel{tab:ch03_iaccompare}{{3.4}{100}{Infrastructure-as-Code tool selection determines deployment reproducibility and operational overhead. Terraform provides declarative multi-cloud support, Pulumi enables programmatic infrastructure with familiar languages, and Ansible offers agentless configuration management. Choose based on cloud provider mix, team expertise, and complexity requirements}{table.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Reusable Modules and Patterns}{100}{subsection.3.4.3}\protected@file@percent }
\newlabel{sec:iac-modules}{{3.4.3}{100}{Reusable Modules and Patterns}{subsection.3.4.3}{}}
\newlabel{lst:ch03_terraform_module}{{3.2}{100}{Reusable Modules and Patterns}{llmlisting.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Compliance, Security, and Auditing}{103}{subsection.3.4.4}\protected@file@percent }
\newlabel{sec:iac-compliance}{{3.4.4}{103}{Compliance, Security, and Auditing}{subsection.3.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Infrastructure Deployment Pipelines}{104}{subsection.3.4.5}\protected@file@percent }
\newlabel{sec:iac-pipelines}{{3.4.5}{104}{Infrastructure Deployment Pipelines}{subsection.3.4.5}{}}
\newlabel{lst:ch03_iac_pipeline}{{3.3}{104}{Infrastructure Deployment Pipelines}{llmlisting.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Documentation as Code}{108}{subsection.3.4.6}\protected@file@percent }
\newlabel{sec:iac-docs}{{3.4.6}{108}{Documentation as Code}{subsection.3.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Checklist: Best Practices for IaC in LLMOps}{109}{subsection.3.4.7}\protected@file@percent }
\newlabel{sec:iac-checklist}{{3.4.7}{109}{Checklist: Best Practices for IaC in LLMOps}{subsection.3.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Containerization and Orchestration}{111}{section.3.5}\protected@file@percent }
\newlabel{sec:orchestration}{{3.5}{111}{Containerization and Orchestration}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Kubernetes for LLMs}{111}{subsection.3.5.1}\protected@file@percent }
\newlabel{sec:orchestration-k8s}{{3.5.1}{111}{Kubernetes for LLMs}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.1}Cluster Architecture, Networking, and Hardening}{112}{subsubsection.3.5.1.1}\protected@file@percent }
\newlabel{sec:k8s-arch-hardening}{{3.5.1.1}{112}{Cluster Architecture, Networking, and Hardening}{subsubsection.3.5.1.1}{}}
\newlabel{lst:ch03_k8s_deployment}{{3.4}{112}{Cluster Architecture, Networking, and Hardening}{llmlisting.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Advanced Scheduling Strategies}{115}{subsection.3.5.2}\protected@file@percent }
\newlabel{sec:orchestration-advanced}{{3.5.2}{115}{Advanced Scheduling Strategies}{subsection.3.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Model Serving Infrastructure}{116}{section.3.6}\protected@file@percent }
\newlabel{sec:serving}{{3.6}{116}{Model Serving Infrastructure}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Serving Frameworks and Engines}{116}{subsection.3.6.1}\protected@file@percent }
\newlabel{sec:serving-fw}{{3.6.1}{116}{Serving Frameworks and Engines}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.1}Hugging Face Text Generation Inference (TGI)}{116}{subsubsection.3.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.2}vLLM}{117}{subsubsection.3.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.3}NVIDIA TensorRT-LLM (with Triton)}{117}{subsubsection.3.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.4}LMDeploy}{117}{subsubsection.3.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.5}SGLang}{118}{subsubsection.3.6.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.6}Other frameworks}{118}{subsubsection.3.6.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Novel Methods for Serving Efficiency}{118}{subsection.3.6.2}\protected@file@percent }
\newlabel{sec:serving-novel}{{3.6.2}{118}{Novel Methods for Serving Efficiency}{subsection.3.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.1}Smoothie Routing (Ensemble Routing)}{118}{subsubsection.3.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.2}KV Cache Compression and Offloading}{118}{subsubsection.3.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.3}Speculative Decoding}{119}{subsubsection.3.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.4}Beyond Beam Search}{119}{subsubsection.3.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.5}Augmented Retrieval Integration}{119}{subsubsection.3.6.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.6}Distributed Serving for Ultra-Large Models}{119}{subsubsection.3.6.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Serving framework selection determines throughput, latency, and operational complexity. Different frameworks optimize for different scenarios: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency inference, and TGI balances ease of use with performance. Choose based on traffic patterns, latency requirements, and team expertise.}}{120}{table.3.5}\protected@file@percent }
\newlabel{tab:ch03_servingcompare}{{3.5}{120}{Serving framework selection determines throughput, latency, and operational complexity. Different frameworks optimize for different scenarios: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency inference, and TGI balances ease of use with performance. Choose based on traffic patterns, latency requirements, and team expertise}{table.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Summary}{120}{subsection.3.6.3}\protected@file@percent }
\newlabel{sec:serving-summary}{{3.6.3}{120}{Summary}{subsection.3.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.3.1}Inference Runtimes as Managed Artifacts}{120}{subsubsection.3.6.3.1}\protected@file@percent }
\newlabel{sec:serving-runtimes}{{3.6.3.1}{120}{Inference Runtimes as Managed Artifacts}{subsubsection.3.6.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Deployment Patterns}{121}{section.3.7}\protected@file@percent }
\newlabel{sec:deploy}{{3.7}{121}{Deployment Patterns}{section.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Deployment pattern selection determines operational complexity and cost structure. \textbf  {Cloud-Native} (top) maximizes elasticity and reduces operational overhead but may increase costs at scale. \textbf  {Hybrid} (middle) balances on-premise control with cloud flexibility, optimizing for cost and compliance. \textbf  {Multi-Cluster} (bottom) enables geographic distribution and failover but increases management complexity. Choose based on scale, compliance requirements, and operational capabilities.}}{122}{figure.3.2}\protected@file@percent }
\newlabel{fig:ch03_deploy_patterns_balanced}{{3.2}{122}{Deployment pattern selection determines operational complexity and cost structure. \textbf {Cloud-Native} (top) maximizes elasticity and reduces operational overhead but may increase costs at scale. \textbf {Hybrid} (middle) balances on-premise control with cloud flexibility, optimizing for cost and compliance. \textbf {Multi-Cluster} (bottom) enables geographic distribution and failover but increases management complexity. Choose based on scale, compliance requirements, and operational capabilities}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Cloud-Native Deployments}{123}{subsection.3.7.1}\protected@file@percent }
\newlabel{sec:deploy-cloud}{{3.7.1}{123}{Cloud-Native Deployments}{subsection.3.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Cloud-native deployment architecture enables scalable, resilient LLMOps. Elastic GPU scaling across multiple availability zones ensures availability; integration with managed services (queues, vector databases, object storage) reduces operational overhead; secure networking through API gateways and private subnets protects sensitive data; disaster recovery capabilities ensure business continuity. GPU nodes auto-scale based on demand, while IAM and security groups enforce access controls. This architecture pattern balances scalability, security, and operational simplicity.}}{124}{figure.3.3}\protected@file@percent }
\newlabel{fig:ch03_cloud_native_deployment}{{3.3}{124}{Cloud-native deployment architecture enables scalable, resilient LLMOps. Elastic GPU scaling across multiple availability zones ensures availability; integration with managed services (queues, vector databases, object storage) reduces operational overhead; secure networking through API gateways and private subnets protects sensitive data; disaster recovery capabilities ensure business continuity. GPU nodes auto-scale based on demand, while IAM and security groups enforce access controls. This architecture pattern balances scalability, security, and operational simplicity}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Hybrid Deployments}{124}{subsection.3.7.2}\protected@file@percent }
\newlabel{sec:deploy-hybrid}{{3.7.2}{124}{Hybrid Deployments}{subsection.3.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Multi-Cluster and Multi-Region Topologies}{125}{subsection.3.7.3}\protected@file@percent }
\newlabel{sec:deploy-multicluster}{{3.7.3}{125}{Multi-Cluster and Multi-Region Topologies}{subsection.3.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}Summary}{126}{subsection.3.7.4}\protected@file@percent }
\newlabel{sec:deploy-summary}{{3.7.4}{126}{Summary}{subsection.3.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Case Study: Ishtar AI Infrastructure}{126}{section.3.8}\protected@file@percent }
\newlabel{sec:ishtar-case}{{3.8}{126}{Case Study: Ishtar AI Infrastructure}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Hardware Mix}{126}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}IaC and Automation}{127}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Kubernetes Configuration}{127}{subsection.3.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.4}Serving Stack}{127}{subsection.3.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.5}Cost and Performance}{128}{subsection.3.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.6}Hybrid Integration}{128}{subsection.3.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.7}Lessons Learned}{128}{subsection.3.8.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Hybrid infrastructure enables cost optimization and operational flexibility for \ishtar  {}. On-prem L4 GPUs handle embeddings and preprocessing, reducing cloud costs for predictable workloads; AWS EKS hosts baseline A100s and autoscaling H100s for summarization, providing elasticity for variable traffic. Terraform, ArgoCD, and Kubernetes manage deployments and scaling, demonstrating how infrastructure-as-code and GitOps enable reliable hybrid operations.}}{129}{figure.3.4}\protected@file@percent }
\newlabel{fig:ch03_ishtar_infra}{{3.4}{129}{Hybrid infrastructure enables cost optimization and operational flexibility for \ishtar {}. On-prem L4 GPUs handle embeddings and preprocessing, reducing cloud costs for predictable workloads; AWS EKS hosts baseline A100s and autoscaling H100s for summarization, providing elasticity for variable traffic. Terraform, ArgoCD, and Kubernetes manage deployments and scaling, demonstrating how infrastructure-as-code and GitOps enable reliable hybrid operations}{figure.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Best Practices and Checklists}{129}{section.3.9}\protected@file@percent }
\newlabel{sec:bestpractices}{{3.9}{129}{Best Practices and Checklists}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Hardware \& Performance Checklist}{129}{subsection.3.9.1}\protected@file@percent }
\newlabel{sec:bp-hw}{{3.9.1}{129}{Hardware \& Performance Checklist}{subsection.3.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}IaC \& DevOps Checklist}{131}{subsection.3.9.2}\protected@file@percent }
\newlabel{sec:bp-iac}{{3.9.2}{131}{IaC \& DevOps Checklist}{subsection.3.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.3}Serving \& Scaling Checklist}{132}{subsection.3.9.3}\protected@file@percent }
\newlabel{sec:bp-serving}{{3.9.3}{132}{Serving \& Scaling Checklist}{subsection.3.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.4}Summary}{132}{subsection.3.9.4}\protected@file@percent }
\newlabel{sec:checklists-summary}{{3.9.4}{132}{Summary}{subsection.3.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Conclusion}{133}{section.3.10}\protected@file@percent }
\newlabel{sec:infra-conclusion}{{3.10}{133}{Conclusion}{section.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.1}Bridging to Part II: Infrastructure as Operational Contracts}{134}{subsection.3.10.1}\protected@file@percent }
\newlabel{sec:infra-part2-bridge}{{3.10.1}{134}{Bridging to Part II: Infrastructure as Operational Contracts}{subsection.3.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.1}Infrastructure Choices Define Operational Contracts}{134}{subsubsection.3.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.2}How Infrastructure Contracts Constrain CI/CD}{135}{subsubsection.3.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.3}How Infrastructure Choices Affect Observability}{135}{subsubsection.3.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.4}How Infrastructure Decisions Impact Scaling}{135}{subsubsection.3.10.1.4}\protected@file@percent }
\@setckpt{ch03-infra-env}{
\setcounter{page}{137}
\setcounter{equation}{3}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{10}
\setcounter{subsection}{1}
\setcounter{subsubsection}{4}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{5}
\setcounter{chapter}{3}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{125}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{49}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{8}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{3}
\setcounter{Item}{12}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
\setcounter{llmlisting}{4}
}

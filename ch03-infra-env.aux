\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Infrastructure and Environment for LLMOps}{75}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:07}{{3}{75}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\newlabel{ch:infra}{{3}{75}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\newlabel{refsegment:08}{{3}{75}{Infrastructure and Environment for LLMOps}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{76}{section.3.1}\protected@file@percent }
\newlabel{sec:ch3-introduction}{{3.1}{76}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Hardware Selection for LLM Workloads}{77}{section.3.2}\protected@file@percent }
\newlabel{sec:infra-hardware-bench}{{3.2}{77}{Hardware Selection for LLM Workloads}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Compute Profiles and Workload Types}{77}{subsection.3.2.1}\protected@file@percent }
\newlabel{sec:infra-hardware-profiles}{{3.2.1}{77}{Compute Profiles and Workload Types}{subsection.3.2.1}{}}
\newlabel{eq:latency-infra}{{3.1}{77}{Compute Profiles and Workload Types}{equation.3.1}{}}
\newlabel{eq:throughput-env}{{3.2}{77}{Compute Profiles and Workload Types}{equation.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces GPU accelerator selection determines cost, throughput, and deployment feasibility. Different accelerators optimize for different model sizes and workloads: A100 provides balanced performance, H100 offers higher throughput and FP8 support, L4 enables cost-effective smaller models. Choose based on model size, traffic volume, and budget constraints.}}{78}{table.3.1}\protected@file@percent }
\newlabel{tab:ch03_gpucompare}{{3.1}{78}{GPU accelerator selection determines cost, throughput, and deployment feasibility. Different accelerators optimize for different model sizes and workloads: A100 provides balanced performance, H100 offers higher throughput and FP8 support, L4 enables cost-effective smaller models. Choose based on model size, traffic volume, and budget constraints}{table.3.1}{}}
\newlabel{eq:kv-memory}{{3.3}{78}{Compute Profiles and Workload Types}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}GPU Architectures and Choices}{78}{subsection.3.2.2}\protected@file@percent }
\newlabel{sec:infra-hardware-gpu}{{3.2.2}{78}{GPU Architectures and Choices}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}TPU Architectures and Considerations}{79}{subsection.3.2.3}\protected@file@percent }
\newlabel{sec:infra-hardware-tpu}{{3.2.3}{79}{TPU Architectures and Considerations}{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Cost Modeling and Economics}{79}{section.3.3}\protected@file@percent }
\newlabel{sec:infra-cost}{{3.3}{79}{Cost Modeling and Economics}{section.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Cost per token varies significantly with utilization and hardware choice. Higher utilization amortizes fixed costs, making H100 cost-effective at scale despite higher hourly rates. Choose A100 for variable workloads with low average utilization; choose H100 for sustained high-throughput deployments where utilization exceeds 60\%.}}{80}{table.3.2}\protected@file@percent }
\newlabel{tab:ch03_cost_per_token}{{3.2}{80}{Cost per token varies significantly with utilization and hardware choice. Higher utilization amortizes fixed costs, making H100 cost-effective at scale despite higher hourly rates. Choose A100 for variable workloads with low average utilization; choose H100 for sustained high-throughput deployments where utilization exceeds 60\%}{table.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Token Economics and Cost per Query}{80}{subsection.3.3.1}\protected@file@percent }
\newlabel{sec:infra-cost-tokens}{{3.3.1}{80}{Token Economics and Cost per Query}{subsection.3.3.1}{}}
\newlabel{eq:costptok}{{3.3.1}{80}{Token Economics and Cost per Query}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Batch Size vs Throughput Trade-offs}{81}{subsection.3.3.2}\protected@file@percent }
\newlabel{sec:infra-cost-batch}{{3.3.2}{81}{Batch Size vs Throughput Trade-offs}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Caching and Quantization Effects}{82}{subsection.3.3.3}\protected@file@percent }
\newlabel{sec:infra-cost-quant}{{3.3.3}{82}{Caching and Quantization Effects}{subsection.3.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.1}KV Cache and Prompt Caching}{82}{subsubsection.3.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Throughput vs.\ batch size trade-off determines capacity planning and latency SLOs. Increasing batch size initially yields large throughput gains (e.g., 3$\times $ improvement from batch 1 to 4), but returns diminish beyond batch size 32. Eventually throughput saturates (batch 64 and 128 achieve similar tokens/sec) as memory or kernel launch overhead becomes the bottleneck. Understanding this saturation point helps teams optimize batch sizes for their latency requirements and hardware constraints.}}{83}{figure.3.1}\protected@file@percent }
\newlabel{fig:ch03_batchtradeoff}{{3.1}{83}{Throughput vs.\ batch size trade-off determines capacity planning and latency SLOs. Increasing batch size initially yields large throughput gains (e.g., 3$\times $ improvement from batch 1 to 4), but returns diminish beyond batch size 32. Eventually throughput saturates (batch 64 and 128 achieve similar tokens/sec) as memory or kernel launch overhead becomes the bottleneck. Understanding this saturation point helps teams optimize batch sizes for their latency requirements and hardware constraints}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.2}Quantization}{84}{subsubsection.3.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.3}Ishtar Case}{84}{subsubsection.3.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Worked Example: Cost per Million Tokens Across Accelerators}{84}{subsection.3.3.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Cost per token varies significantly across accelerators and utilization levels. Lower-cost accelerators (L4) become cost-effective for smaller models or variable workloads, while high-end accelerators (H100) justify their premium at high utilization with large models. When cost differences exceed 2$\times $, hardware choice becomes a primary cost optimization lever. Assumes typical sustained throughput for a 13B--70B model and on-demand cloud pricing.}}{85}{table.3.3}\protected@file@percent }
\newlabel{tab:ch03_costcompare}{{3.3}{85}{Cost per token varies significantly across accelerators and utilization levels. Lower-cost accelerators (L4) become cost-effective for smaller models or variable workloads, while high-end accelerators (H100) justify their premium at high utilization with large models. When cost differences exceed 2$\times $, hardware choice becomes a primary cost optimization lever. Assumes typical sustained throughput for a 13B--70B model and on-demand cloud pricing}{table.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Infrastructure-as-Code (IaC) for LLMOps}{85}{section.3.4}\protected@file@percent }
\newlabel{sec:iac}{{3.4}{85}{Infrastructure-as-Code (IaC) for LLMOps}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Why IaC Matters}{85}{subsection.3.4.1}\protected@file@percent }
\newlabel{sec:iac-why}{{3.4.1}{85}{Why IaC Matters}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tooling Comparison}{86}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:iac-tools}{{3.4.2}{86}{Tooling Comparison}{subsection.3.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Infrastructure-as-Code tool selection determines deployment reproducibility and operational overhead. Terraform provides declarative multi-cloud support, Pulumi enables programmatic infrastructure with familiar languages, and Ansible offers agentless configuration management. Choose based on cloud provider mix, team expertise, and complexity requirements.}}{87}{table.3.4}\protected@file@percent }
\newlabel{tab:ch03_iaccompare}{{3.4}{87}{Infrastructure-as-Code tool selection determines deployment reproducibility and operational overhead. Terraform provides declarative multi-cloud support, Pulumi enables programmatic infrastructure with familiar languages, and Ansible offers agentless configuration management. Choose based on cloud provider mix, team expertise, and complexity requirements}{table.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Reusable Modules and Patterns}{88}{subsection.3.4.3}\protected@file@percent }
\newlabel{sec:iac-modules}{{3.4.3}{88}{Reusable Modules and Patterns}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Compliance, Security, and Auditing}{88}{subsection.3.4.4}\protected@file@percent }
\newlabel{sec:iac-compliance}{{3.4.4}{88}{Compliance, Security, and Auditing}{subsection.3.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Infrastructure Deployment Pipelines}{88}{subsection.3.4.5}\protected@file@percent }
\newlabel{sec:iac-pipelines}{{3.4.5}{88}{Infrastructure Deployment Pipelines}{subsection.3.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Documentation as Code}{89}{subsection.3.4.6}\protected@file@percent }
\newlabel{sec:iac-docs}{{3.4.6}{89}{Documentation as Code}{subsection.3.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Checklist: Best Practices for IaC in LLMOps}{89}{subsection.3.4.7}\protected@file@percent }
\newlabel{sec:iac-checklist}{{3.4.7}{89}{Checklist: Best Practices for IaC in LLMOps}{subsection.3.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Containerization and Orchestration}{90}{section.3.5}\protected@file@percent }
\newlabel{sec:orchestration}{{3.5}{90}{Containerization and Orchestration}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Kubernetes for LLMs}{90}{subsection.3.5.1}\protected@file@percent }
\newlabel{sec:orchestration-k8s}{{3.5.1}{90}{Kubernetes for LLMs}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.1}Cluster Architecture, Networking, and Hardening}{91}{subsubsection.3.5.1.1}\protected@file@percent }
\newlabel{sec:k8s-arch-hardening}{{3.5.1.1}{91}{Cluster Architecture, Networking, and Hardening}{subsubsection.3.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Advanced Scheduling Strategies}{92}{subsection.3.5.2}\protected@file@percent }
\newlabel{sec:orchestration-advanced}{{3.5.2}{92}{Advanced Scheduling Strategies}{subsection.3.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Model Serving Infrastructure}{93}{section.3.6}\protected@file@percent }
\newlabel{sec:serving}{{3.6}{93}{Model Serving Infrastructure}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Serving Frameworks and Engines}{93}{subsection.3.6.1}\protected@file@percent }
\newlabel{sec:serving-fw}{{3.6.1}{93}{Serving Frameworks and Engines}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.1}Hugging Face Text Generation Inference (TGI)}{93}{subsubsection.3.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.2}vLLM}{93}{subsubsection.3.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.3}NVIDIA TensorRT-LLM (with Triton)}{94}{subsubsection.3.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.4}LMDeploy}{94}{subsubsection.3.6.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.5}SGLang}{94}{subsubsection.3.6.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1.6}Other frameworks}{94}{subsubsection.3.6.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Novel Methods for Serving Efficiency}{95}{subsection.3.6.2}\protected@file@percent }
\newlabel{sec:serving-novel}{{3.6.2}{95}{Novel Methods for Serving Efficiency}{subsection.3.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.1}Smoothie Routing (Ensemble Routing)}{95}{subsubsection.3.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.2}KV Cache Compression and Offloading}{95}{subsubsection.3.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.3}Speculative Decoding}{95}{subsubsection.3.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.4}Beyond Beam Search}{95}{subsubsection.3.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.5}Augmented Retrieval Integration}{96}{subsubsection.3.6.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2.6}Distributed Serving for Ultra-Large Models}{96}{subsubsection.3.6.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Summary}{96}{subsection.3.6.3}\protected@file@percent }
\newlabel{sec:serving-summary}{{3.6.3}{96}{Summary}{subsection.3.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.3.1}Inference Runtimes as Managed Artifacts}{96}{subsubsection.3.6.3.1}\protected@file@percent }
\newlabel{sec:serving-runtimes}{{3.6.3.1}{96}{Inference Runtimes as Managed Artifacts}{subsubsection.3.6.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Serving framework selection determines throughput, latency, and operational complexity. Different frameworks optimize for different scenarios: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency inference, and TGI balances ease of use with performance. Choose based on traffic patterns, latency requirements, and team expertise.}}{97}{table.3.5}\protected@file@percent }
\newlabel{tab:ch03_servingcompare}{{3.5}{97}{Serving framework selection determines throughput, latency, and operational complexity. Different frameworks optimize for different scenarios: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency inference, and TGI balances ease of use with performance. Choose based on traffic patterns, latency requirements, and team expertise}{table.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Deployment Patterns}{97}{section.3.7}\protected@file@percent }
\newlabel{sec:deploy}{{3.7}{97}{Deployment Patterns}{section.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Cloud-Native Deployments}{97}{subsection.3.7.1}\protected@file@percent }
\newlabel{sec:deploy-cloud}{{3.7.1}{97}{Cloud-Native Deployments}{subsection.3.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Hybrid Deployments}{98}{subsection.3.7.2}\protected@file@percent }
\newlabel{sec:deploy-hybrid}{{3.7.2}{98}{Hybrid Deployments}{subsection.3.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Cloud-native deployment architecture enables scalable, resilient LLMOps. Elastic GPU scaling across multiple availability zones ensures availability; integration with managed services (queues, vector databases, object storage) reduces operational overhead; secure networking through API gateways and private subnets protects sensitive data; disaster recovery capabilities ensure business continuity. GPU nodes auto-scale based on demand, while IAM and security groups enforce access controls. This architecture pattern balances scalability, security, and operational simplicity.}}{99}{figure.3.2}\protected@file@percent }
\newlabel{fig:ch03_cloud_native_deployment}{{3.2}{99}{Cloud-native deployment architecture enables scalable, resilient LLMOps. Elastic GPU scaling across multiple availability zones ensures availability; integration with managed services (queues, vector databases, object storage) reduces operational overhead; secure networking through API gateways and private subnets protects sensitive data; disaster recovery capabilities ensure business continuity. GPU nodes auto-scale based on demand, while IAM and security groups enforce access controls. This architecture pattern balances scalability, security, and operational simplicity}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Multi-Cluster and Multi-Region Topologies}{100}{subsection.3.7.3}\protected@file@percent }
\newlabel{sec:deploy-multicluster}{{3.7.3}{100}{Multi-Cluster and Multi-Region Topologies}{subsection.3.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Deployment pattern selection determines operational complexity and cost structure. \textbf  {Cloud-Native} (left) maximizes elasticity and reduces operational overhead but may increase costs at scale. \textbf  {Hybrid} (middle) balances on-premise control with cloud flexibility, optimizing for cost and compliance. \textbf  {Multi-Cluster} (right) enables geographic distribution and failover but increases management complexity. Choose based on scale, compliance requirements, and operational capabilities.}}{101}{figure.3.3}\protected@file@percent }
\newlabel{fig:ch03_deploy_patterns_balanced}{{3.3}{101}{Deployment pattern selection determines operational complexity and cost structure. \textbf {Cloud-Native} (left) maximizes elasticity and reduces operational overhead but may increase costs at scale. \textbf {Hybrid} (middle) balances on-premise control with cloud flexibility, optimizing for cost and compliance. \textbf {Multi-Cluster} (right) enables geographic distribution and failover but increases management complexity. Choose based on scale, compliance requirements, and operational capabilities}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}Summary}{101}{subsection.3.7.4}\protected@file@percent }
\newlabel{sec:deploy-summary}{{3.7.4}{101}{Summary}{subsection.3.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Case Study: Ishtar AI Infrastructure}{101}{section.3.8}\protected@file@percent }
\newlabel{sec:ishtar-case}{{3.8}{101}{Case Study: Ishtar AI Infrastructure}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Hardware Mix}{102}{subsection.3.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}IaC and Automation}{102}{subsection.3.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Kubernetes Configuration}{102}{subsection.3.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.4}Serving Stack}{103}{subsection.3.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.5}Cost and Performance}{103}{subsection.3.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.6}Hybrid Integration}{103}{subsection.3.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.7}Lessons Learned}{103}{subsection.3.8.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Hybrid infrastructure enables cost optimization and operational flexibility for \ishtar  {}. On-prem L4 GPUs handle embeddings and preprocessing, reducing cloud costs for predictable workloads; AWS EKS hosts baseline A100s and autoscaling H100s for summarization, providing elasticity for variable traffic. Terraform, ArgoCD, and Kubernetes manage deployments and scaling, demonstrating how infrastructure-as-code and GitOps enable reliable hybrid operations.}}{104}{figure.3.4}\protected@file@percent }
\newlabel{fig:ch03_ishtar_infra}{{3.4}{104}{Hybrid infrastructure enables cost optimization and operational flexibility for \ishtar {}. On-prem L4 GPUs handle embeddings and preprocessing, reducing cloud costs for predictable workloads; AWS EKS hosts baseline A100s and autoscaling H100s for summarization, providing elasticity for variable traffic. Terraform, ArgoCD, and Kubernetes manage deployments and scaling, demonstrating how infrastructure-as-code and GitOps enable reliable hybrid operations}{figure.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Best Practices and Checklists}{104}{section.3.9}\protected@file@percent }
\newlabel{sec:bestpractices}{{3.9}{104}{Best Practices and Checklists}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Hardware \& Performance Checklist}{105}{subsection.3.9.1}\protected@file@percent }
\newlabel{sec:bp-hw}{{3.9.1}{105}{Hardware \& Performance Checklist}{subsection.3.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}IaC \& DevOps Checklist}{106}{subsection.3.9.2}\protected@file@percent }
\newlabel{sec:bp-iac}{{3.9.2}{106}{IaC \& DevOps Checklist}{subsection.3.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.3}Serving \& Scaling Checklist}{107}{subsection.3.9.3}\protected@file@percent }
\newlabel{sec:bp-serving}{{3.9.3}{107}{Serving \& Scaling Checklist}{subsection.3.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.4}Summary}{107}{subsection.3.9.4}\protected@file@percent }
\newlabel{sec:checklists-summary}{{3.9.4}{107}{Summary}{subsection.3.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Conclusion}{108}{section.3.10}\protected@file@percent }
\newlabel{sec:infra-conclusion}{{3.10}{108}{Conclusion}{section.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.1}Bridging to Part II: Infrastructure as Operational Contracts}{109}{subsection.3.10.1}\protected@file@percent }
\newlabel{sec:infra-part2-bridge}{{3.10.1}{109}{Bridging to Part II: Infrastructure as Operational Contracts}{subsection.3.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.1}Infrastructure Choices Define Operational Contracts}{109}{subsubsection.3.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.2}How Infrastructure Contracts Constrain CI/CD}{109}{subsubsection.3.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.3}How Infrastructure Choices Affect Observability}{110}{subsubsection.3.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1.4}How Infrastructure Decisions Impact Scaling}{110}{subsubsection.3.10.1.4}\protected@file@percent }
\@setckpt{ch03-infra-env}{
\setcounter{page}{114}
\setcounter{equation}{3}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{10}
\setcounter{subsection}{1}
\setcounter{subsubsection}{4}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{5}
\setcounter{chapter}{3}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{1}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{11}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{267}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{8}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{9}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{3}
\setcounter{Item}{12}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
}

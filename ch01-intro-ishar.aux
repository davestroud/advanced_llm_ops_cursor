\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to LLMOps and the Ishtar AI Case Study}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:02}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{ch:intro}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{refsegment:03}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1.1}{5}{Introduction}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Operational Challenges}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:operational-challenges}{{1.2}{7}{Operational Challenges}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Compute Economics: Cost, Latency, and Capacity}{7}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Latency decomposition and scale levers for LLM serving. End-to-end latency is the sum of stage latencies (tokenization/policy checks, retrieval, LLM prefill+decode, post-processing). Sustaining throughput and cost efficiency requires coordinated controls: batching and scheduling for throughput, caching/quantization/routing for unit economics, and explicit capacity planning (peak demand, tail latency, and autoscaling on queue depth).}}{8}{figure.1.1}\protected@file@percent }
\newlabel{fig:cost-latency-throughput}{{1.1}{8}{Latency decomposition and scale levers for LLM serving. End-to-end latency is the sum of stage latencies (tokenization/policy checks, retrieval, LLM prefill+decode, post-processing). Sustaining throughput and cost efficiency requires coordinated controls: batching and scheduling for throughput, caching/quantization/routing for unit economics, and explicit capacity planning (peak demand, tail latency, and autoscaling on queue depth)}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Serving Infrastructure and Systems Engineering}{8}{subsection.1.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Architecture of an LLM-powered application showing retrieval-augmented generation (RAG) flow with fallback mechanisms. The orchestrator manages user queries, optionally retrieves relevant documents from a knowledge base via a vector database, constructs prompts for an LLM, allows the LLM to use external tools, and finally delivers a coherent response to the user.}}{9}{figure.1.2}\protected@file@percent }
\newlabel{fig:ch01:architecture}{{1.2}{9}{Architecture of an LLM-powered application showing retrieval-augmented generation (RAG) flow with fallback mechanisms. The orchestrator manages user queries, optionally retrieves relevant documents from a knowledge base via a vector database, constructs prompts for an LLM, allows the LLM to use external tools, and finally delivers a coherent response to the user}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Data and Knowledge Drift (Especially in RAG)}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces RAG drift management as a closed-loop operational process: detect drift signals, diagnose the cause, remediate via index/retriever/governance changes, and validate with canary evaluation plus index/config snapshots for auditability and rollback.}}{10}{figure.1.3}\protected@file@percent }
\newlabel{fig:rag-drift-control}{{1.3}{10}{RAG drift management as a closed-loop operational process: detect drift signals, diagnose the cause, remediate via index/retriever/governance changes, and validate with canary evaluation plus index/config snapshots for auditability and rollback}{figure.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Evaluation: From Single Metrics to Behavioral Guarantees}{10}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Observability and Debuggability}{11}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Security, Privacy, and New Threat Models}{12}{subsection.1.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces LLMOps observability: what to capture beyond traditional monitoring, why it matters, and common failure signals.}}{13}{table.1.1}\protected@file@percent }
\newlabel{tab:llm_observability}{{1.1}{13}{LLMOps observability: what to capture beyond traditional monitoring, why it matters, and common failure signals}{table.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Security for tool- and retrieval-augmented LLM systems. Threats (prompt injection, policy bypass, retrieval-based exfiltration, tool misuse, and output leakage) map to concrete controls (input sanitization, instruction hierarchy, retrieval governance, tool sandboxing, and output redaction), with audit-grade tracing as a cross-cutting requirement for incident response and rollback.}}{14}{figure.1.4}\protected@file@percent }
\newlabel{fig:llm-threat-model}{{1.4}{14}{Security for tool- and retrieval-augmented LLM systems. Threats (prompt injection, policy bypass, retrieval-based exfiltration, tool misuse, and output leakage) map to concrete controls (input sanitization, instruction hierarchy, retrieval governance, tool sandboxing, and output redaction), with audit-grade tracing as a cross-cutting requirement for incident response and rollback}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Change Management and Release Discipline}{14}{subsection.1.2.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Four dimensions where LLMOps extends MLOps.}}{15}{table.1.2}\protected@file@percent }
\newlabel{tab:llmops-extends-mlops}{{1.2}{15}{Four dimensions where LLMOps extends MLOps}{table.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.8}Why This Motivates LLMOps}{15}{subsection.1.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Infrastructure and Environment Design}{16}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Emergence of LLMOps}{17}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}This Book and the Ishtar AI Case Study}{17}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}From MLOps to LLMOps: Evolution and Key Differences}{17}{section.1.6}\protected@file@percent }
\newlabel{sec:mlops-to-llmops}{{1.6}{17}{From MLOps to LLMOps: Evolution and Key Differences}{section.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces From MLOps to LLMOps: modeling milestones and operational inflections. Major capability jumps (top) correlate with new operational requirements (bottom): serving at scale, rigorous evaluation/red-teaming, and governance/policy embedded into the lifecycle.}}{18}{figure.1.5}\protected@file@percent }
\newlabel{fig:mlops-llmops-timeline}{{1.5}{18}{From MLOps to LLMOps: modeling milestones and operational inflections. Major capability jumps (top) correlate with new operational requirements (bottom): serving at scale, rigorous evaluation/red-teaming, and governance/policy embedded into the lifecycle}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Why LLMOps is Distinct}{19}{subsection.1.6.1}\protected@file@percent }
\newlabel{sec:why-llmops-distinct}{{1.6.1}{19}{Why LLMOps is Distinct}{subsection.1.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.1}Scale}{19}{subsubsection.1.6.1.1}\protected@file@percent }
\newlabel{sec:llmops-scale}{{1.6.1.1}{19}{Scale}{subsubsection.1.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Why scale changes LLM deployment. Large parameter counts make weight memory a first-order constraint, while longer contexts introduce $\Theta (T^2)$ attention cost and KV-cache pressure. Together these drive LLM-specific SLOs (TTFT, tokens/s, \$/1K tokens) and engineering levers (parallelism, quantization, batching/scheduling, KV-cache policy, and model routing).}}{20}{figure.1.6}\protected@file@percent }
\newlabel{fig:scale-constraints}{{1.6}{20}{Why scale changes LLM deployment. Large parameter counts make weight memory a first-order constraint, while longer contexts introduce $\Theta (T^2)$ attention cost and KV-cache pressure. Together these drive LLM-specific SLOs (TTFT, tokens/s, \$/1K tokens) and engineering levers (parallelism, quantization, batching/scheduling, KV-cache policy, and model routing)}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.2}Complexity}{20}{subsubsection.1.6.1.2}\protected@file@percent }
\newlabel{sec:llmops-complexity}{{1.6.1.2}{20}{Complexity}{subsubsection.1.6.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.3}Variability}{21}{subsubsection.1.6.1.3}\protected@file@percent }
\newlabel{sec:llmops-variability}{{1.6.1.3}{21}{Variability}{subsubsection.1.6.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.4}Risk and Alignment}{21}{subsubsection.1.6.1.4}\protected@file@percent }
\newlabel{sec:llmops-risk-alignment}{{1.6.1.4}{21}{Risk and Alignment}{subsubsection.1.6.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Summary}{21}{subsection.1.6.2}\protected@file@percent }
\newlabel{sec:why-llmops-summary}{{1.6.2}{21}{Summary}{subsection.1.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Structure of the Book}{22}{section.1.7}\protected@file@percent }
\newlabel{sec:book-structure}{{1.7}{22}{Structure of the Book}{section.1.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Operational role of each chapter in the LLMOps lifecycle.}}{23}{table.1.3}\protected@file@percent }
\newlabel{tab:llmops-legend}{{1.3}{23}{Operational role of each chapter in the LLMOps lifecycle}{table.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces LLMOps lifecycle mapped to the book's chapters. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices.}}{24}{figure.1.7}\protected@file@percent }
\newlabel{fig:llmops-lifecycle}{{1.7}{24}{LLMOps lifecycle mapped to the book's chapters. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}How to Read This Book}{24}{subsection.1.7.1}\protected@file@percent }
\newlabel{subsec:how-to-read}{{1.7.1}{24}{How to Read This Book}{subsection.1.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Introducing the Ishtar AI Case Study}{24}{section.1.8}\protected@file@percent }
\newlabel{sec:ishtar-intro}{{1.8}{24}{Introducing the Ishtar AI Case Study}{section.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Mini legend for Fig.~\ref {fig:llmops-lifecycle}: why each chapter matters operationally. \emph  {Legend of chapter roles in the lifecycle.}}}{25}{figure.1.8}\protected@file@percent }
\newlabel{fig:llmops-legend}{{1.8}{25}{Mini legend for Fig.~\ref {fig:llmops-lifecycle}: why each chapter matters operationally. \emph {Legend of chapter roles in the lifecycle.}}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Purpose of \ishtar  {}}{2
\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to LLMOps and the Ishtar AI Case Study}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:03}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{ch:intro}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{refsegment:04}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{6}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1.1}{6}{Introduction}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Operational Challenges}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:operational-challenges}{{1.2}{7}{Operational Challenges}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Compute Economics: Cost, Latency, and Capacity}{8}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Serving Infrastructure and Systems Engineering}{8}{subsection.1.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Latency decomposition enables targeted optimization and capacity planning. End-to-end latency is the sum of stage latencies (tokenization/policy checks, retrieval, LLM prefill+decode, post-processing), and identifying bottlenecks requires measuring each stage independently. Sustaining throughput and cost efficiency demands coordinated controls: batching and scheduling optimize throughput, caching/quantization/routing improve unit economics, and explicit capacity planning (peak demand, tail latency, autoscaling) ensures SLO compliance.}}{9}{figure.1.1}\protected@file@percent }
\newlabel{fig:ch01_cost_latency_throughput}{{1.1}{9}{Latency decomposition enables targeted optimization and capacity planning. End-to-end latency is the sum of stage latencies (tokenization/policy checks, retrieval, LLM prefill+decode, post-processing), and identifying bottlenecks requires measuring each stage independently. Sustaining throughput and cost efficiency demands coordinated controls: batching and scheduling optimize throughput, caching/quantization/routing improve unit economics, and explicit capacity planning (peak demand, tail latency, autoscaling) ensures SLO compliance}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Data and Knowledge Drift (Especially in RAG)}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces LLM application architecture determines reliability and capability. This RAG-based design enables grounded responses through retrieval, fallback mechanisms ensure graceful degradation, and tool integration extends model capabilities. This architecture pattern balances accuracy (via retrieval), reliability (via fallbacks), and functionality (via tools) for production deployments.}}{10}{figure.1.2}\protected@file@percent }
\newlabel{fig:ch01_architecture}{{1.2}{10}{LLM application architecture\index {architecture!LLM application} determines reliability and capability. This RAG\index {RAG|see{Retrieval-Augmented Generation}}\index {Retrieval-Augmented Generation|(}-based design enables grounded responses through retrieval, fallback mechanisms ensure graceful degradation, and tool integration extends model capabilities. This architecture pattern balances accuracy (via retrieval), reliability (via fallbacks), and functionality (via tools) for production deployments}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces RAG drift management as a closed-loop operational process: detect drift signals, diagnose the cause, remediate via index/retriever/governance changes, and validate with canary evaluation plus index/config snapshots for auditability and rollback.}}{11}{figure.1.3}\protected@file@percent }
\newlabel{fig:ch01_rag_drift_control}{{1.3}{11}{RAG drift management as a closed-loop operational process: detect drift signals, diagnose the cause, remediate via index/retriever/governance changes, and validate with canary evaluation plus index/config snapshots for auditability and rollback}{figure.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Evaluation: From Single Metrics to Behavioral Guarantees}{11}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Observability and Debuggability}{12}{subsection.1.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces LLMOps observability: what to capture beyond traditional monitoring, why it matters, and common failure signals.}}{13}{table.1.1}\protected@file@percent }
\newlabel{tab:ch01_llm_observability}{{1.1}{13}{LLMOps observability: what to capture beyond traditional monitoring, why it matters, and common failure signals}{table.1.1}{}}
\newlabel{lst:ch01_observability_schema}{{1.1}{13}{Observability and Debuggability}{llmlisting.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Security, Privacy, and New Threat Models}{15}{subsection.1.2.6}\protected@file@percent }
\newlabel{lst:ch01_tool_schema}{{1.2}{16}{Security, Privacy, and New Threat Models}{llmlisting.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Security architecture for tool- and retrieval-augmented LLM systems requires defense-in-depth. Threats (prompt injection, policy bypass, retrieval-based exfiltration, tool misuse, and output leakage) map to concrete controls (input sanitization, instruction hierarchy, retrieval governance, tool sandboxing, and output redaction), with audit-grade tracing as a cross-cutting requirement for incident response and rollback. This threat model guides security architecture decisions and demonstrates why traditional application security is insufficient for LLM systems.}}{18}{figure.1.4}\protected@file@percent }
\newlabel{fig:ch01_llm_threat_model}{{1.4}{18}{Security architecture\index {security!architecture} for tool- and retrieval-augmented LLM systems requires defense-in-depth\index {defense-in-depth}. Threats (prompt injection, policy bypass\index {policy bypass}, retrieval-based exfiltration\index {data exfiltration}, tool misuse\index {tool misuse}, and output leakage\index {output leakage}) map to concrete controls (input sanitization\index {input sanitization}, instruction hierarchy\index {instruction hierarchy}, retrieval governance\index {retrieval!governance}, tool sandboxing\index {sandboxing}, and output redaction\index {output redaction}), with audit-grade tracing\index {audit trail} as a cross-cutting requirement for incident response and rollback. This threat model guides security architecture decisions and demonstrates why traditional application security is insufficient for LLM systems}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Change Management and Release Discipline}{19}{subsection.1.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.8}Why This Motivates LLMOps}{19}{subsection.1.2.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces LLMOps extends MLOps in four critical dimensions that require new operational practices. Scale introduces memory and cost constraints; complexity demands semantic observability; variability requires prompt and retrieval testing; risk necessitates safety gates and policy enforcement. Understanding these extensions helps teams anticipate operational challenges and plan infrastructure investments.}}{20}{table.1.2}\protected@file@percent }
\newlabel{tab:ch01_llmops_extends_mlops}{{1.2}{20}{LLMOps extends MLOps in four critical dimensions that require new operational practices. Scale introduces memory\index {memory} and cost constraints; complexity demands semantic observability; variability requires prompt and retrieval testing; risk necessitates safety gates\index {safety gate} and policy enforcement\index {policy enforcement}. Understanding these extensions helps teams anticipate operational challenges and plan infrastructure investments}{table.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Infrastructure and Environment Design}{21}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Emergence of LLMOps}{21}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}This Book and the Ishtar AI Case Study}{22}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}From MLOps to LLMOps: Evolution and Key Differences}{22}{section.1.6}\protected@file@percent }
\newlabel{sec:mlops-to-llmops}{{1.6}{22}{From MLOps to LLMOps: Evolution and Key Differences}{section.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces LLMOps evolution reflects changing operational requirements. As model capabilities increased (transformer scaling, instruction following, tool use), operational needs shifted from basic serving to rigorous evaluation, safety gates, and governance. Understanding these inflections helps teams anticipate operational complexity and plan infrastructure investments.}}{23}{figure.1.5}\protected@file@percent }
\newlabel{fig:ch01_mlops_llmops_timeline}{{1.5}{23}{LLMOps evolution reflects changing operational requirements. As model capabilities increased (transformer scaling, instruction following, tool use), operational needs shifted from basic serving to rigorous evaluation, safety gates, and governance. Understanding these inflections helps teams anticipate operational complexity and plan infrastructure investments}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Why LLMOps is Distinct}{23}{subsection.1.6.1}\protected@file@percent }
\newlabel{sec:why-llmops-distinct}{{1.6.1}{23}{Why LLMOps is Distinct}{subsection.1.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.1}Scale}{23}{subsubsection.1.6.1.1}\protected@file@percent }
\newlabel{sec:llmops-scale}{{1.6.1.1}{23}{Scale}{subsubsection.1.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Why scale changes LLM deployment. Large parameter counts make weight memory a first-order constraint, while longer contexts introduce $\Theta (T^2)$ attention cost and KV-cache pressure. Together these drive LLM-specific SLOs (TTFT, tokens/s, \$/1K tokens) and engineering levers (parallelism, quantization, batching/scheduling, KV-cache policy, and model routing). Understanding these constraints enables teams to make informed infrastructure decisions and set realistic performance targets.}}{24}{figure.1.6}\protected@file@percent }
\newlabel{fig:ch01_scale_constraints}{{1.6}{24}{Why scale changes LLM deployment. Large parameter counts make weight memory a first-order constraint, while longer contexts introduce $\Theta (T^2)$ attention cost and KV-cache pressure. Together these drive LLM-specific SLOs (TTFT, tokens/s, \$/1K tokens) and engineering levers (parallelism, quantization, batching/scheduling, KV-cache policy, and model routing). Understanding these constraints enables teams to make informed infrastructure decisions and set realistic performance targets}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.2}Complexity}{25}{subsubsection.1.6.1.2}\protected@file@percent }
\newlabel{sec:llmops-complexity}{{1.6.1.2}{25}{Complexity}{subsubsection.1.6.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.3}Variability}{25}{subsubsection.1.6.1.3}\protected@file@percent }
\newlabel{sec:llmops-variability}{{1.6.1.3}{25}{Variability}{subsubsection.1.6.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.4}Risk and Alignment}{25}{subsubsection.1.6.1.4}\protected@file@percent }
\newlabel{sec:llmops-risk-alignment}{{1.6.1.4}{25}{Risk and Alignment}{subsubsection.1.6.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Summary}{26}{subsection.1.6.2}\protected@file@percent }
\newlabel{sec:why-llmops-summary}{{1.6.2}{26}{Summary}{subsection.1.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Structure of the Book}{26}{section.1.7}\protected@file@percent }
\newlabel{sec:book-structure}{{1.7}{26}{Structure of the Book}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}How to Read This Book}{27}{subsection.1.7.1}\protected@file@percent }
\newlabel{subsec:how-to-read}{{1.7.1}{27}{How to Read This Book}{subsection.1.7.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Chapter organization maps to operational lifecycle stages. Each chapter addresses specific operational challenges (foundations, delivery, optimization, governance), enabling readers to understand both individual practices and their integration into a complete LLMOps system. This organization supports both sequential reading and targeted reference for specific operational needs.}}{28}{table.1.3}\protected@file@percent }
\newlabel{tab:ch01_llmops_legend}{{1.3}{28}{Chapter organization maps to operational lifecycle stages. Each chapter addresses specific operational challenges (foundations, delivery, optimization, governance), enabling readers to understand both individual practices and their integration into a complete LLMOps system. This organization supports both sequential reading and targeted reference for specific operational needs}{table.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces LLMOps lifecycle demonstrates how operational practices integrate across development stages. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices, illustrating how real-world experience informs operational improvements.}}{29}{figure.1.7}\protected@file@percent }
\newlabel{fig:ch01_llmops_lifecycle}{{1.7}{29}{LLMOps lifecycle demonstrates how operational practices integrate across development stages. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices, illustrating how real-world experience informs operational improvements}{figure.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Introducing the Ishtar AI Case Study}{29}{section.1.8}\protected@file@percent }
\newlabel{sec:ishtar-intro}{{1.8}{29}{Introducing the Ishtar AI Case Study}{section.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Purpose of \ishtar  {}}{29}{subsection.1.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Chapter roles explain operational contributions to the LLMOps lifecycle. This legend clarifies why each chapter matters operationally, helping readers understand how individual practices (e.g., CI/CD, observability, scaling) contribute to overall system reliability and performance. See Fig.~\ref {fig:ch01_llmops_lifecycle} for the complete lifecycle visualization.}}{30}{figure.1.8}\protected@file@percent }
\newlabel{fig:ch01_llmops_legend}{{1.8}{30}{Chapter roles explain operational contributions to the LLMOps lifecycle. This legend clarifies why each chapter matters operationally, helping readers understand how individual practices (e.g., CI/CD, observability, scaling) contribute to overall system reliability and performance. See Fig.~\ref {fig:ch01_llmops_lifecycle} for the complete lifecycle visualization}{figure.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Subsystem-to-chapter mapping guides readers through \ishtar  {}'s architecture. Each subsystem (ingestion, retrieval, orchestration, inference) corresponds to specific chapters, enabling readers to understand both the architecture and the operational practices that make it production-ready. This mapping demonstrates how book concepts apply to real-world systems.}}{31}{figure.1.9}\protected@file@percent }
\newlabel{fig:ch01_ishtar_arch_callout}{{1.9}{31}{Subsystem-to-chapter mapping guides readers through \ishtar {}'s architecture. Each subsystem (ingestion, retrieval, orchestration, inference) corresponds to specific chapters, enabling readers to understand both the architecture and the operational practices that make it production-ready. This mapping demonstrates how book concepts apply to real-world systems}{figure.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Architecture Overview}{31}{subsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.1}Data Ingestion}{31}{subsubsection.1.8.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces \ishtar  {} reference architecture demonstrates production-ready LLMOps patterns. The pipeline integrates data ingestion, retrieval-augmented generation, multi-agent orchestration, and GPU-backed inference, with observability spanning all stages. This architecture balances accuracy (via RAG), reliability (via multi-agent verification), and scalability (via GPU inference), serving as a template for similar deployments.}}{32}{figure.1.10}\protected@file@percent }
\newlabel{fig:ch01_ishtar_arch_main}{{1.10}{32}{\ishtar {} reference architecture demonstrates production-ready LLMOps patterns. The pipeline integrates data ingestion, retrieval-augmented generation, multi-agent orchestration, and GPU-backed inference, with observability spanning all stages. This architecture balances accuracy (via RAG), reliability (via multi-agent verification), and scalability (via GPU inference), serving as a template for similar deployments}{figure.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.2}Retrieval-Augmented Generation (RAG)}{32}{subsubsection.1.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.3}Multi-Agent Orchestration}{32}{subsubsection.1.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.4}Inference Cluster}{33}{subsubsection.1.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.5}Observability and Feedback}{33}{subsubsection.1.8.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.3}LLMOps in Practice}{33}{subsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Core Components of LLMOps}{33}{section.1.9}\protected@file@percent }
\newlabel{sec:core-llmops}{{1.9}{33}{Core Components of LLMOps}{section.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Evidence source selection determines RAG quality and trustworthiness. \ishtar  {} curates inputs from news wires, NGO bulletins, vetted social media, and public health \& infrastructure feeds, ensuring reliable, authoritative sources. This curation strategy demonstrates how source quality directly impacts answer accuracy and citation fidelity in production RAG systems.}}{34}{figure.1.11}\protected@file@percent }
\newlabel{fig:ch01_ishtar_inputs_taxonomy}{{1.11}{34}{Evidence source selection determines RAG quality and trustworthiness. \ishtar {} curates inputs from news wires, NGO bulletins, vetted social media, and public health \& infrastructure feeds, ensuring reliable, authoritative sources. This curation strategy demonstrates how source quality directly impacts answer accuracy and citation fidelity in production RAG systems}{figure.1.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}Prompt Management}{34}{subsection.1.9.1}\protected@file@percent }
\newlabel{subsec:prompt-mgmt}{{1.9.1}{34}{Prompt Management}{subsection.1.9.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.1.1}Objectives}{34}{subsubsection.1.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.1.2}Practices}{34}{subsubsection.1.9.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.1.3}Example}{35}{subsubsection.1.9.1.3}\protected@file@percent }
\newlabel{lst:ch01_prompt_template}{{1.3}{35}{Example}{llmlisting.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.2}Retrieval and RAG Pipelines}{36}{subsection.1.9.2}\protected@file@percent }
\newlabel{subsec:rag}{{1.9.2}{36}{Retrieval and RAG Pipelines}{subsection.1.9.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.2.1}Design choices}{36}{subsubsection.1.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.2.2}Operational concerns}{36}{subsubsection.1.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.2.3}Example}{37}{subsubsection.1.9.2.3}\protected@file@percent }
\newlabel{lst:ch01_rag_configuration}{{1.4}{37}{Example}{llmlisting.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.3}Deployment and Serving}{38}{subsection.1.9.3}\protected@file@percent }
\newlabel{subsec:serving}{{1.9.3}{38}{Deployment and Serving}{subsection.1.9.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.3.1}Serving stack}{38}{subsubsection.1.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.3.2}Release engineering}{39}{subsubsection.1.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.3.3}Example}{39}{subsubsection.1.9.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.4}Evaluation and Testing}{39}{subsection.1.9.4}\protected@file@percent }
\newlabel{subsec:evaluation}{{1.9.4}{39}{Evaluation and Testing}{subsection.1.9.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.1}Evaluation layers}{39}{subsubsection.1.9.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.2}Regression control}{39}{subsubsection.1.9.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.3}Example}{40}{subsubsection.1.9.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.4}Systems telemetry}{40}{subsubsection.1.9.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.5}Model telemetry}{40}{subsubsection.1.9.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.6}Tooling and alerts}{40}{subsubsection.1.9.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.9.4.7}Example}{40}{subsubsection.1.9.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}LLMOps in Practice: Successes, Failures, and Lessons Learned}{41}{section.1.10}\protected@file@percent }
\newlabel{sec:llmops-practice}{{1.10}{41}{LLMOps in Practice: Successes, Failures, and Lessons Learned}{section.1.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Preview of Subsequent Chapters}{43}{section.1.11}\protected@file@percent }
\newlabel{sec:preview}{{1.11}{43}{Preview of Subsequent Chapters}{section.1.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces Early LLM deployments reveal critical operational lessons. Failures (Galactica, Sydney) demonstrate the cost of inadequate safety gates and evaluation; successes (Character.AI) show the value of systematic testing and monitoring. These case studies illustrate why operational discipline is essential for responsible LLM deployment.}}{44}{table.1.4}\protected@file@percent }
\newlabel{tab:ch01_early_llmops_cases}{{1.4}{44}{Early LLM deployments reveal critical operational lessons. Failures (Galactica, Sydney) demonstrate the cost of inadequate safety gates and evaluation; successes (Character.AI) show the value of systematic testing and monitoring. These case studies illustrate why operational discipline is essential for responsible LLM deployment}{table.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces LLMOps monitoring checklist enables proactive operations. This quick reference highlights key signals (latency, cost, quality, safety) that teams must track to maintain production reliability. Regular monitoring of these signals enables early detection of regressions and supports data-driven optimization decisions.}}{46}{figure.1.12}\protected@file@percent }
\newlabel{fig:ch01_llmops_quick_checklist}{{1.12}{46}{LLMOps monitoring checklist enables proactive operations. This quick reference highlights key signals (latency, cost, quality, safety) that teams must track to maintain production reliability. Regular monitoring of these signals enables early detection of regressions and supports data-driven optimization decisions}{figure.1.12}{}}
\@setckpt{ch01-intro-ishar}{
\setcounter{page}{50}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{11}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{4}
\setcounter{chapter}{1}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{60}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{12}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{78}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{4}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{9}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{1}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
\setcounter{llmlisting}{4}
}

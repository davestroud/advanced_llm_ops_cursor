\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to LLMOps and the Ishtar AI Case Study}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:02}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{ch:intro}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\newlabel{refsegment:03}{{1}{5}{Introduction to LLMOps and the Ishtar AI Case Study}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1.1}{5}{Introduction}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The LLMOps continuous improvement lifecycle, showing how design, deployment, operation, and monitoring feed into iterative refinement.}}{7}{figure.1.1}\protected@file@percent }
\newlabel{fig:ch01:lifecycle}{{1.1}{7}{The LLMOps continuous improvement lifecycle, showing how design, deployment, operation, and monitoring feed into iterative refinement}{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Operational Challenges}{7}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Compute Economics: Cost, Latency, and Capacity}{7}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Serving Infrastructure and Systems Engineering}{8}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Data and Knowledge Drift (Especially in RAG)}{8}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Evaluation: From Single Metrics to Behavioral Guarantees}{8}{subsection.1.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Architecture of an LLM-powered application showing retrieval-augmented generation (RAG) flow with fallback mechanisms. The orchestrator manages user queries, optionally retrieves relevant documents from a knowledge base via a vector database, constructs prompts for an LLM, allows the LLM to use external tools, and finally delivers a coherent response to the user.}}{9}{figure.1.2}\protected@file@percent }
\newlabel{fig:ch01:architecture}{{1.2}{9}{Architecture of an LLM-powered application showing retrieval-augmented generation (RAG) flow with fallback mechanisms. The orchestrator manages user queries, optionally retrieves relevant documents from a knowledge base via a vector database, constructs prompts for an LLM, allows the LLM to use external tools, and finally delivers a coherent response to the user}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Observability and Debuggability}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Security, Privacy, and New Threat Models}{10}{subsection.1.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Change Management and Release Discipline}{10}{subsection.1.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.8}Cost, Latency, and Throughput at Scale}{10}{subsection.1.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.9}Infrastructure and Serving Complexity}{11}{subsection.1.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.10}Data, Drift, and Feedback Loops}{11}{subsection.1.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.11}Evaluation and Quality Assurance}{11}{subsection.1.2.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.12}Observability Beyond Traditional Monitoring}{12}{subsection.1.2.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.13}Security, Privacy, and Policy Enforcement}{12}{subsection.1.2.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.14}Why This Motivates LLMOps}{12}{subsection.1.2.14}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Four dimensions where LLMOps extends MLOps.}}{13}{table.1.1}\protected@file@percent }
\newlabel{tab:llmops-extends-mlops}{{1.1}{13}{Four dimensions where LLMOps extends MLOps}{table.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Infrastructure and Environment Design}{14}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Emergence of LLMOps}{14}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}This Book and the Ishtar AI Case Study}{15}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}From MLOps to LLMOps: Evolution and Key Differences}{15}{section.1.6}\protected@file@percent }
\newlabel{sec:mlops-to-llmops}{{1.6}{15}{From MLOps to LLMOps: Evolution and Key Differences}{section.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces From MLOps to LLMOps: modeling milestones and operational inflections. Major capability jumps (top) correlate with new operational requirements (bottom): serving at scale, rigorous evaluation/red-teaming, and governance/policy embedded into the lifecycle.}}{16}{figure.1.3}\protected@file@percent }
\newlabel{fig:mlops-llmops-timeline}{{1.3}{16}{From MLOps to LLMOps: modeling milestones and operational inflections. Major capability jumps (top) correlate with new operational requirements (bottom): serving at scale, rigorous evaluation/red-teaming, and governance/policy embedded into the lifecycle}{figure.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Why LLMOps is Distinct}{16}{subsection.1.6.1}\protected@file@percent }
\newlabel{sec:why-llmops-distinct}{{1.6.1}{16}{Why LLMOps is Distinct}{subsection.1.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.1}Scale}{16}{subsubsection.1.6.1.1}\protected@file@percent }
\newlabel{sec:llmops-scale}{{1.6.1.1}{16}{Scale}{subsubsection.1.6.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.2}Complexity}{17}{subsubsection.1.6.1.2}\protected@file@percent }
\newlabel{sec:llmops-complexity}{{1.6.1.2}{17}{Complexity}{subsubsection.1.6.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.3}Variability}{17}{subsubsection.1.6.1.3}\protected@file@percent }
\newlabel{sec:llmops-variability}{{1.6.1.3}{17}{Variability}{subsubsection.1.6.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1.4}Risk and Alignment}{18}{subsubsection.1.6.1.4}\protected@file@percent }
\newlabel{sec:llmops-risk-alignment}{{1.6.1.4}{18}{Risk and Alignment}{subsubsection.1.6.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Summary}{18}{subsection.1.6.2}\protected@file@percent }
\newlabel{sec:why-llmops-summary}{{1.6.2}{18}{Summary}{subsection.1.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Structure of the Book}{18}{section.1.7}\protected@file@percent }
\newlabel{sec:book-structure}{{1.7}{18}{Structure of the Book}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}How to Read This Book}{19}{subsection.1.7.1}\protected@file@percent }
\newlabel{subsec:how-to-read}{{1.7.1}{19}{How to Read This Book}{subsection.1.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces LLMOps lifecycle mapped to the book's chapters. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices.}}{20}{figure.1.4}\protected@file@percent }
\newlabel{fig:llmops-lifecycle}{{1.4}{20}{LLMOps lifecycle mapped to the book's chapters. The flow progresses from foundations (Part I) through delivery operations (Part II: CI/CD, monitoring, scaling), optimization (Part III: performance, RAG, multi-agent), and governance (Part IV: testing, ethics, case study). A dashed feedback loop connects the case study back to foundational practices}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Mini legend for Fig.~\ref {fig:llmops-lifecycle}: why each chapter matters operationally. \emph  {Legend of chapter roles in the lifecycle.}}}{20}{figure.1.5}\protected@file@percent }
\newlabel{fig:llmops-legend}{{1.5}{20}{Mini legend for Fig.~\ref {fig:llmops-lifecycle}: why each chapter matters operationally. \emph {Legend of chapter roles in the lifecycle.}}{figure.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Introducing the Ishtar AI Case Study}{21}{section.1.8}\protected@file@percent }
\newlabel{sec:ishtar-intro}{{1.8}{21}{Introducing the Ishtar AI Case Study}{section.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Purpose of \ishtar  {}}{21}{subsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Architecture Overview}{21}{subsection.1.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Subsystem-to-chapter mapping for the \ishtar  {} reference architecture.}}{22}{figure.1.6}\protected@file@percent }
\newlabel{fig:ishtar-arch-callout}{{1.6}{22}{Subsystem-to-chapter mapping for the \ishtar {} reference architecture}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces \ishtar  {} reference architecture. The pipeline includes data ingestion, retrieval-augmented generation, multi-agent orchestration, and GPU-backed inference, with observability spanning all stages.}}{22}{figure.1.7}\protected@file@percent }
\newlabel{fig:ishtar-arch-main}{{1.7}{22}{\ishtar {} reference architecture. The pipeline includes data ingestion, retrieval-augmented generation, multi-agent orchestration, and GPU-backed inference, with observability spanning all stages}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.1}Data Ingestion}{22}{subsubsection.1.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.2}Retrieval-Augmented Generation (RAG)}{22}{subsubsection.1.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.3}Multi-Agent Orchestration}{22}{subsubsection.1.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Evidence sources powering \ishtar  {}. Curated inputs (news wires, NGO bulletins, vetted social media, and public health \& infrastructure feeds) are routed through retrieval to support journalist queries.}}{23}{figure.1.8}\protected@file@percent }
\newlabel{fig:ishtar-inputs-taxonomy}{{1.8}{23}{Evidence sources powering \ishtar {}. Curated inputs (news wires, NGO bulletins, vetted social media, and public health \& infrastructure feeds) are routed through retrieval to support journalist queries}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.4}Inference Cluster}{23}{subsubsection.1.8.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2.5}Observability and Feedback}{23}{subsubsection.1.8.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.3}LLMOps in Practice}{23}{subsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Core Components of LLMOps}{24}{section.1.9}\protected@file@percent }
\newlabel{sec:core-llmops}{{1.9}{24}{Core Components of LLMOps}{section.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}Prompt Management}{24}{subsection.1.9.1}\protected@file@percent }
\newlabel{subsec:prompt-mgmt}{{1.9.1}{24}{Prompt Management}{subsection.1.9.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numbe
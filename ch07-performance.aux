\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Performance Optimization Strategies for LLMs}{191}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:014}{{7}{191}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\newlabel{ch:performance}{{7}{191}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\newlabel{refsegment:015}{{7}{191}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Why Optimization Matters}{192}{section.7.1}\protected@file@percent }
\newlabel{sec:perf-why}{{7.1}{192}{Why Optimization Matters}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Representative performance improvements from widely deployed techniques, based on published benchmarks and production deployments. Actual gains depend on model size, hardware, and traffic mix. See Sections 7.1--7.4 for detailed discussions and citations.}}{193}{figure.7.1}\protected@file@percent }
\newlabel{fig:opt-gains-callout}{{7.1}{193}{Representative performance improvements from widely deployed techniques, based on published benchmarks and production deployments. Actual gains depend on model size, hardware, and traffic mix. See Sections 7.1--7.4 for detailed discussions and citations}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Illustrative latency improvements after batching, KV-cache engineering, and quantization. Replace with your measured values to reflect your deployment.}}{193}{figure.7.2}\protected@file@percent }
\newlabel{fig:latency-before-after}{{7.2}{193}{Illustrative latency improvements after batching, KV-cache engineering, and quantization. Replace with your measured values to reflect your deployment}{figure.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Model-Level Optimization Techniques}{193}{section.7.2}\protected@file@percent }
\newlabel{sec:perf-model}{{7.2}{193}{Model-Level Optimization Techniques}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Quantization}{194}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1.1}Pros:}{194}{subsubsection.7.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1.2}Cons:}{194}{subsubsection.7.2.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Comparison of common quantization schemes for large language models.}}{195}{table.7.1}\protected@file@percent }
\newlabel{tab:quantization-comparison}{{7.1}{195}{Comparison of common quantization schemes for large language models}{table.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Pruning}{195}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Knowledge Distillation}{196}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Efficient Fine-Tuning}{196}{subsection.7.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Illustrative throughput gains from quantization on A100/H100 (normalized to FP16). Replace values with your measured tokens/s to reflect your deployment. FP8 is native on H100; 4-bit performance varies by toolkit/model.}}{197}{figure.7.3}\protected@file@percent }
\newlabel{fig:quant-throughput-bars}{{7.3}{197}{Illustrative throughput gains from quantization on A100/H100 (normalized to FP16). Replace values with your measured tokens/s to reflect your deployment. FP8 is native on H100; 4-bit performance varies by toolkit/model}{figure.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Inference Engine Optimization}{197}{section.7.3}\protected@file@percent }
\newlabel{sec:perf-engine}{{7.3}{197}{Inference Engine Optimization}{section.7.3}{}}
\newlabel{RF2}{198}
\@w
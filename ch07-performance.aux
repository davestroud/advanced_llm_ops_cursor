\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Performance Optimization Strategies for LLMs}{287}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:015}{{7}{287}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\newlabel{ch:performance}{{7}{287}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\newlabel{refsegment:016}{{7}{287}{Performance Optimization Strategies for LLMs}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{288}{section.7.1}\protected@file@percent }
\newlabel{sec:ch7-introduction}{{7.1}{288}{Introduction}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Why Optimization Matters}{288}{section.7.2}\protected@file@percent }
\newlabel{sec:perf-why}{{7.2}{288}{Why Optimization Matters}{section.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Model-Level Optimization Techniques}{289}{section.7.3}\protected@file@percent }
\newlabel{sec:perf-model}{{7.3}{289}{Model-Level Optimization Techniques}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Quantization}{289}{subsection.7.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Performance optimization techniques deliver substantial gains but require careful selection. Quantization, batching, and caching can each provide 2--4$\times $ improvements, but gains are multiplicative and depend on workload characteristics. Understanding these improvement ranges helps teams prioritize optimization efforts and set realistic performance targets. See Sections~\ref {sec:perf-model}--\ref {sec:perf-system} and Section~\ref {sec:perf-prompt} for detailed discussions and implementation guidance.}}{290}{figure.7.1}\protected@file@percent }
\newlabel{fig:ch07_opt_gains_callout}{{7.1}{290}{Performance optimization techniques deliver substantial gains but require careful selection. Quantization, batching, and caching can each provide 2--4$\times $ improvements, but gains are multiplicative and depend on workload characteristics. Understanding these improvement ranges helps teams prioritize optimization efforts and set realistic performance targets. See Sections~\ref {sec:perf-model}--\ref {sec:perf-system} and Section~\ref {sec:perf-prompt} for detailed discussions and implementation guidance}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Optimization techniques deliver substantial latency improvements when applied systematically. Batching amortizes overhead, KV-cache engineering reduces memory access, and quantization increases compute speed. Combined, these techniques can reduce latency by 2--4$\times $, but gains are multiplicative and depend on workload characteristics. Absolute values vary by model, hardware, and traffic mix.}}{291}{figure.7.2}\protected@file@percent }
\newlabel{fig:ch07_latency_before_after}{{7.2}{291}{Optimization techniques deliver substantial latency improvements when applied systematically. Batching amortizes overhead, KV-cache engineering reduces memory access, and quantization increases compute speed. Combined, these techniques can reduce latency by 2--4$\times $, but gains are multiplicative and depend on workload characteristics. Absolute values vary by model, hardware, and traffic mix}{figure.7.2}{}}
\newlabel{lst:ch07_quantization_config}{{7.1}{292}{Quantization}{llmlisting.7.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Quantization scheme selection trades off accuracy, speed, and memory. INT8 provides 2$\times $ speedup with minimal accuracy loss; 4-bit quantization enables 4$\times $ memory reduction but requires calibration; FP8 offers native hardware support on H100. Choose based on model sensitivity, hardware capabilities, and accuracy requirements.}}{294}{table.7.1}\protected@file@percent }
\newlabel{tab:ch07_quantization_comparison}{{7.1}{294}{Quantization scheme selection trades off accuracy, speed, and memory. INT8 provides 2$\times $ speedup with minimal accuracy loss; 4-bit quantization enables 4$\times $ memory reduction but requires calibration; FP8 offers native hardware support on H100. Choose based on model sensitivity, hardware capabilities, and accuracy requirements}{table.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Pruning}{294}{subsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Knowledge Distillation}{295}{subsection.7.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Efficient Fine-Tuning}{296}{subsection.7.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Illustrative throughput gains from quantization on A100/H100 (normalized to FP16). FP8 is native on H100; 4-bit performance varies by toolkit and model architecture.}}{297}{figure.7.3}\protected@file@percent }
\newlabel{fig:ch07_quant_throughput_bars}{{7.3}{297}{Illustrative throughput gains from quantization on A100/H100 (normalized to FP16). FP8 is native on H100; 4-bit performance varies by toolkit and model architecture}{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Memory accounting enables capacity planning and batch size optimization. Use precision $b$ from Table~\ref {tab:ch07_quantization_comparison} and model dimensions from your architecture to size weights and KV cache before setting batch/sequence budgets. This accounting prevents OOM errors and helps teams understand memory headroom for batching and context length.}}{297}{figure.7.4}\protected@file@percent }
\newlabel{fig:ch07_memory_cheatsheet}{{7.4}{297}{Memory accounting enables capacity planning and batch size optimization. Use precision $b$ from Table~\ref {tab:ch07_quantization_comparison} and model dimensions from your architecture to size weights and KV cache before setting batch/sequence budgets. This accounting prevents OOM errors and helps teams understand memory headroom for batching and context length}{figure.7.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Compression and adaptation techniques enable efficient LLM deployment. Quantization reduces memory and increases speed; distillation transfers knowledge to smaller models; LoRA enables parameter-efficient fine-tuning. Choose based on deployment constraints, accuracy requirements, and update frequency.}}{298}{table.7.2}\protected@file@percent }
\newlabel{tab:ch07_compression_summary}{{7.2}{298}{Compression and adaptation techniques enable efficient LLM deployment. Quantization reduces memory and increases speed; distillation transfers knowledge to smaller models; LoRA enables parameter-efficient fine-tuning. Choose based on deployment constraints, accuracy requirements, and update frequency}{table.7.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Precision selection determines memory requirements and deployment feasibility. Lower precision (INT8, 4-bit) dramatically reduces memory, enabling larger models on fixed hardware or higher batch sizes. However, precision reduction may impact accuracy, requiring careful evaluation. Understanding these trade-offs enables teams to optimize memory usage while maintaining quality.}}{299}{table.7.3}\protected@file@percent }
\newlabel{tab:ch07_mem_vs_precision}{{7.3}{299}{Precision selection determines memory requirements and deployment feasibility. Lower precision (INT8, 4-bit) dramatically reduces memory, enabling larger models on fixed hardware or higher batch sizes. However, precision reduction may impact accuracy, requiring careful evaluation. Understanding these trade-offs enables teams to optimize memory usage while maintaining quality}{table.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Inference Engine Optimization}{300}{section.7.4}\protected@file@percent }
\newlabel{sec:perf-engine}{{7.4}{300}{Inference Engine Optimization}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Specialized Runtimes}{300}{subsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Operator Fusion}{300}{subsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Paged Attention}{301}{subsection.7.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Inference engine selection significantly impacts throughput and latency. Different engines optimize for different workloads: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency single-request performance, and TGI balances both. Choose based on traffic patterns, latency requirements, and hardware constraints.}}{303}{table.7.4}\protected@file@percent }
\newlabel{tab:ch07_engine_bench}{{7.4}{303}{Inference engine selection significantly impacts throughput and latency. Different engines optimize for different workloads: vLLM excels at high-throughput batching, TensorRT-LLM provides low-latency single-request performance, and TGI balances both. Choose based on traffic patterns, latency requirements, and hardware constraints}{table.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}System-Level Optimization}{303}{section.7.5}\protected@file@percent }
\newlabel{sec:perf-system}{{7.5}{303}{System-Level Optimization}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Batching Strategies}{303}{subsection.7.5.1}\protected@file@percent }
\newlabel{sec:perf-batching}{{7.5.1}{303}{Batching Strategies}{subsection.7.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Throughput scaling with batch size reveals optimization opportunities and limits. Throughput rises rapidly at small $B$ as kernel-launch and memory overheads are amortized, then saturates once the accelerator becomes compute- or bandwidth-bound. Understanding this saturation point helps teams optimize batch sizes, but in production, the optimal $B$ is constrained by latency SLOs and KV-cache memory.}}{304}{figure.7.5}\protected@file@percent }
\newlabel{fig:ch07_throughput_vs_batch}{{7.5}{304}{Throughput scaling with batch size reveals optimization opportunities and limits. Throughput rises rapidly at small $B$ as kernel-launch and memory overheads are amortized, then saturates once the accelerator becomes compute- or bandwidth-bound. Understanding this saturation point helps teams optimize batch sizes, but in production, the optimal $B$ is constrained by latency SLOs and KV-cache memory}{figure.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Asynchronous Processing}{305}{subsection.7.5.2}\protected@file@percent }
\newlabel{sec:perf-async}{{7.5.2}{305}{Asynchronous Processing}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Caching}{306}{subsection.7.5.3}\protected@file@percent }
\newlabel{sec:perf-cache}{{7.5.3}{306}{Caching}{subsection.7.5.3}{}}
\newlabel{lst:ch07_caching_config}{{7.2}{307}{Caching}{llmlisting.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Prompt Optimization}{310}{section.7.6}\protected@file@percent }
\newlabel{sec:perf-prompt}{{7.6}{310}{Prompt Optimization}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Reducing Context Size}{310}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Template Efficiency}{311}{subsection.7.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Compression of Retrieved Context}{311}{subsection.7.6.3}\protected@file@percent }
\newlabel{sec:perf-rag-compression}{{7.6.3}{311}{Compression of Retrieved Context}{subsection.7.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Speculative Decoding}{312}{subsection.7.6.4}\protected@file@percent }
\newlabel{sec:perf-speculative}{{7.6.4}{312}{Speculative Decoding}{subsection.7.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Hardware Utilization Tuning}{313}{section.7.7}\protected@file@percent }
\newlabel{sec:perf-hw}{{7.7}{313}{Hardware Utilization Tuning}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}GPU Profiling}{313}{subsection.7.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Mixed Precision}{314}{subsection.7.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Concurrency Tuning}{315}{subsection.7.7.3}\protected@file@percent }
\newlabel{lst:ch07_profiling_script}{{7.3}{316}{Concurrency Tuning}{llmlisting.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Kernel-time profiling identifies optimization opportunities and validates improvements. Comparing breakdowns before vs.\ after hardware utilization tuning (e.g., kernel fusion, mixed precision, and overlap of I/O with compute) reveals which optimizations deliver the largest gains. This profiling guides optimization prioritization and helps teams understand where further improvements are possible. Percentages vary by model, sequence length, and runtime.}}{321}{table.7.5}\protected@file@percent }
\newlabel{tab:ch07_profiling_breakdown}{{7.5}{321}{Kernel-time profiling identifies optimization opportunities and validates improvements. Comparing breakdowns before vs.\ after hardware utilization tuning (e.g., kernel fusion, mixed precision, and overlap of I/O with compute) reveals which optimizations deliver the largest gains. This profiling guides optimization prioritization and helps teams understand where further improvements are possible. Percentages vary by model, sequence length, and runtime}{table.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Performance Testing and Benchmarking}{321}{section.7.8}\protected@file@percent }
\newlabel{sec:perf-benchmark}{{7.8}{321}{Performance Testing and Benchmarking}{section.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Latency CDF reveals tail latency behavior critical for user experience. While p50 latency determines average user experience, p95/p99 percentiles expose tail latency that frustrates users and violates SLOs. Shape and percentiles vary with model size, prompt length, batching policy, and traffic mix, making CDF analysis essential for capacity planning and optimization prioritization.}}{323}{figure.7.6}\protected@file@percent }
\newlabel{fig:ch07_latency_cdf}{{7.6}{323}{Latency CDF reveals tail latency behavior critical for user experience. While p50 latency determines average user experience, p95/p99 percentiles expose tail latency that frustrates users and violates SLOs. Shape and percentiles vary with model size, prompt length, batching policy, and traffic mix, making CDF analysis essential for capacity planning and optimization prioritization}{figure.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Cost per token varies significantly across inference engines and deployment configurations. Engine selection, hardware choice, and utilization levels can create 2--5$\times $ cost differences. When cost differences exceed 30\%, engine choice becomes a primary cost optimization lever, especially at scale. See Section~\ref {sec:perf-cost-per-token} for detailed cost modeling. Absolute values depend on hardware pricing, utilization, and batch/sequence lengths.}}{323}{figure.7.7}\protected@file@percent }
\newlabel{fig:ch07_cost_per_1k_by_engine}{{7.7}{323}{Cost per token varies significantly across inference engines and deployment configurations. Engine selection, hardware choice, and utilization levels can create 2--5$\times $ cost differences. When cost differences exceed 30\%, engine choice becomes a primary cost optimization lever, especially at scale. See Section~\ref {sec:perf-cost-per-token} for detailed cost modeling. Absolute values depend on hardware pricing, utilization, and batch/sequence lengths}{figure.7.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Case Study: Optimizing Ishtar AI}{324}{section.7.9}\protected@file@percent }
\newlabel{sec:perf-ishtar-case}{{7.9}{324}{Case Study: Optimizing Ishtar AI}{section.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.1}Initial Performance}{324}{subsection.7.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.2}Optimizations Applied}{324}{subsection.7.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.9.2.1}Model quantization to INT8/4-bit.}{324}{subsubsection.7.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.9.2.2}Inference engine swap (vLLM with dynamic batching).}{325}{subsubsection.7.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.9.2.3}Prompt and context optimization (RAG compression).}{325}{subsubsection.7.9.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.9.2.4}Asynchronous API and streaming.}{326}{subsubsection.7.9.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.3}Results}{326}{subsection.7.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.9.3.1}Alternate configurations considered.}{326}{subsubsection.7.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Best Practices Checklist (Quick)}{327}{section.7.10}\protected@file@percent }
\newlabel{sec:perf-checklist-quick}{{7.10}{327}{Best Practices Checklist (Quick)}{section.7.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Best Practices Checklist}{327}{section.7.11}\protected@file@percent }
\newlabel{sec:perf-checklist}{{7.11}{327}{Best Practices Checklist}{section.7.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.6}{\ignorespaces \ishtar  {} optimization KPIs demonstrate systematic improvement through targeted techniques. Comparing before vs.\ after metrics (latency, throughput, cost) validates that optimization strategies deliver measurable gains. These KPIs provide benchmarks for similar deployments and illustrate how systematic optimization achieves production goals. Values are representative of the case study scenario and depend on workload and hardware.}}{328}{table.7.6}\protected@file@percent }
\newlabel{tab:ch07_ishtar_kpis}{{7.6}{328}{\ishtar {} optimization KPIs demonstrate systematic improvement through targeted techniques. Comparing before vs.\ after metrics (latency, throughput, cost) validates that optimization strategies deliver measurable gains. These KPIs provide benchmarks for similar deployments and illustrate how systematic optimization achieves production goals. Values are representative of the case study scenario and depend on workload and hardware}{table.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Extended Material}{330}{section.7.12}\protected@file@percent }
\newlabel{sec:perf-extended}{{7.12}{330}{Extended Material}{section.7.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}Architectural Variants and Cloud Deployment Trade-offs}{330}{subsection.7.12.1}\protected@file@percent }
\newlabel{sec:perf-variants}{{7.12.1}{330}{Architectural Variants and Cloud Deployment Trade-offs}{subsection.7.12.1}{}}
\newlabel{sec:arch-variants}{{7.12.1}{330}{Architectural Variants and Cloud Deployment Trade-offs}{subsection.7.12.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}Encoder-Only (Masked LM)}{330}{subsection.7.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.3}Decoder-Only (Autoregressive LM)}{330}{subsection.7.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.4}Mixture-of-Experts (MoE)}{331}{subsection.7.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.4.1}Guideline.}{332}{subsubsection.7.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.5}Complexity and Scaling: Cost Models and Memory Formulas}{332}{subsection.7.12.5}\protected@file@percent }
\newlabel{sec:perf-cost-models}{{7.12.5}{332}{Complexity and Scaling: Cost Models and Memory Formulas}{subsection.7.12.5}{}}
\newlabel{sec:complexity-scaling}{{7.12.5}{332}{Complexity and Scaling: Cost Models and Memory Formulas}{subsection.7.12.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.6}Attention and KV Cache}{332}{subsection.7.12.6}\protected@file@percent }
\newlabel{sec:perf-attention-kv-cache}{{7.12.6}{332}{Attention and KV Cache}{subsection.7.12.6}{}}
\newlabel{eq:kv-layer}{{7.2}{332}{Attention and KV Cache}{equation.7.2}{}}
\newlabel{eq:kv-total}{{7.3}{332}{Attention and KV Cache}{equation.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.6.1}Worked Example.}{333}{subsubsection.7.12.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.7}Throughput and Utilization}{333}{subsection.7.12.7}\protected@file@percent }
\newlabel{eq:throughput-main}{{7.4}{333}{Throughput and Utilization}{equation.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.7.1}Interpretation.}{333}{subsubsection.7.12.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.8}Cost per 1{,}000 Tokens}{334}{subsection.7.12.8}\protected@file@percent }
\newlabel{sec:perf-cost-per-token}{{7.12.8}{334}{Cost per 1{,}000 Tokens}{subsection.7.12.8}{}}
\newlabel{eq:cost-k}{{7.5}{334}{Cost per 1{,}000 Tokens}{equation.7.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.8.1}Worked Example.}{334}{subsubsection.7.12.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.9}Inference Engines and Serving Runtimes}{334}{subsection.7.12.9}\protected@file@percent }
\newlabel{sec:inference-engines}{{7.12.9}{334}{Inference Engines and Serving Runtimes}{subsection.7.12.9}{}}
\newlabel{lst:ch07_inference_engine}{{7.4}{334}{Inference Engines and Serving Runtimes}{llmlisting.7.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.7}{\ignorespaces Architectural variant selection determines cost, latency, and operational complexity. Different deployment patterns (serverless, containerized, edge) optimize for different scenarios: serverless reduces operational overhead but may increase latency; containerized provides control but requires infrastructure management. Choose based on traffic patterns, latency requirements, and team capabilities.}}{335}{table.7.7}\protected@file@percent }
\newlabel{tab:ch07_arch_comparison}{{7.7}{335}{Architectural variant selection determines cost, latency, and operational complexity. Different deployment patterns (serverless, containerized, edge) optimize for different scenarios: serverless reduces operational overhead but may increase latency; containerized provides control but requires infrastructure management. Choose based on traffic patterns, latency requirements, and team capabilities}{table.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.9.1}Practice notes.}{337}{subsubsection.7.12.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.9.2}Discussion.}{337}{subsubsection.7.12.9.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.8}{\ignorespaces Serving runtime selection determines operational complexity and performance characteristics. Different runtimes optimize for different deployment scenarios: some prioritize ease of use, others focus on maximum throughput or lowest latency. Choose based on team expertise, scale requirements, and operational constraints.}}{338}{table.7.8}\protected@file@percent }
\newlabel{tab:ch07_runtimes}{{7.8}{338}{Serving runtime selection determines operational complexity and performance characteristics. Different runtimes optimize for different deployment scenarios: some prioritize ease of use, others focus on maximum throughput or lowest latency. Choose based on team expertise, scale requirements, and operational constraints}{table.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.10}Cloud-Native Optimization Patterns}{339}{subsection.7.12.10}\protected@file@percent }
\newlabel{sec:perf-cloud-native}{{7.12.10}{339}{Cloud-Native Optimization Patterns}{subsection.7.12.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.11}Right-Sizing and Instance Mix}{339}{subsection.7.12.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.12}Autoscaling and Queuing}{339}{subsection.7.12.12}\protected@file@percent }
\newlabel{sec:perf-autoscaling-queue}{{7.12.12}{339}{Autoscaling and Queuing}{subsection.7.12.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.13}Model Parallelism vs.\ Replication}{339}{subsection.7.12.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.14}I/O and Storage}{339}{subsection.7.12.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.15}LangChain-Centric Performance Engineering}{340}{subsection.7.12.15}\protected@file@percent }
\newlabel{sec:perf-langchain}{{7.12.15}{340}{LangChain-Centric Performance Engineering}{subsection.7.12.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.16}Tracing, Telemetry, and Token Accounting}{340}{subsection.7.12.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.17}Caching and Deterministic Subchains}{340}{subsection.7.12.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.18}Model Routing and Cascades}{340}{subsection.7.12.18}\protected@file@percent }
\newlabel{sec:perf-model-routing}{{7.12.18}{340}{Model Routing and Cascades}{subsection.7.12.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.19}Failure Budgeting and Retries}{340}{subsection.7.12.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.20}Extended Case Study: Ishtar AI}{340}{subsection.7.12.20}\protected@file@percent }
\newlabel{sec:perf-ishtar-extended}{{7.12.20}{340}{Extended Case Study: Ishtar AI}{subsection.7.12.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.20.1}Setup.}{340}{subsubsection.7.12.20.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.20.2}Interventions.}{341}{subsubsection.7.12.20.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.12.20.3}Outcomes.}{341}{subsubsection.7.12.20.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.21}Implementation Checklist (Addendum)}{341}{subsection.7.12.21}\protected@file@percent }
\newlabel{sec:perf-impl-checklist}{{7.12.21}{341}{Implementation Checklist (Addendum)}{subsection.7.12.21}{}}
\@setckpt{ch07-performance}{
\setcounter{page}{345}
\setcounter{equation}{5}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{3}
\setcounter{section}{12}
\setcounter{subsection}{21}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{8}
\setcounter{chapter}{7}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{102}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{150}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{763}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{16}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{9}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{2}
\setcounter{Item}{54}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
\setcounter{llmlisting}{4}
}

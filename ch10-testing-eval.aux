\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Testing, Evaluation, and System Robustness}{323}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{refsegment:021}{{10}{323}{Testing, Evaluation, and System Robustness}{chapter.10}{}}
\newlabel{ch:testing}{{10}{323}{Testing, Evaluation, and System Robustness}{chapter.10}{}}
\newlabel{refsegment:022}{{10}{323}{Testing, Evaluation, and System Robustness}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Introduction}{324}{section.10.1}\protected@file@percent }
\newlabel{sec:ch10-introduction}{{10.1}{324}{Introduction}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}The Importance of Testing in LLMOps}{325}{section.10.2}\protected@file@percent }
\newlabel{sec:ch10-the-importance-of-testing-in-llmops}{{10.2}{325}{The Importance of Testing in LLMOps}{section.10.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Types of Testing}{326}{section.10.3}\protected@file@percent }
\newlabel{sec:ch10-types-of-testing}{{10.3}{326}{Types of Testing}{section.10.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Unit Testing}{326}{subsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Integration Testing}{326}{subsection.10.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Testing pyramid prioritization balances feedback speed with coverage depth. Lower layers (unit tests) run frequently and provide fast feedback for frequently changing artifacts; upper layers (end-to-end, adversarial) run less often but validate system-level behavior and security. This pyramid structure enables teams to catch regressions early while maintaining comprehensive validation.}}{327}{figure.10.1}\protected@file@percent }
\newlabel{fig:ch10_testing_pyramid}{{10.1}{327}{Testing pyramid prioritization balances feedback speed with coverage depth. Lower layers (unit tests) run frequently and provide fast feedback for frequently changing artifacts; upper layers (end-to-end, adversarial) run less often but validate system-level behavior and security. This pyramid structure enables teams to catch regressions early while maintaining comprehensive validation}{figure.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}End-to-End Testing}{327}{subsection.10.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.4}Adversarial Testing}{327}{subsection.10.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A practical testing pyramid for LLM systems.}{327}{section*.81}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Test layer selection determines coverage and feedback speed. Different layers (unit, integration, end-to-end, adversarial) validate different aspects: unit tests catch component regressions early; integration tests verify cross-component behavior; end-to-end tests validate user workflows; adversarial tests ensure security. Understanding what each layer validates helps teams design comprehensive test suites.}}{328}{table.10.1}\protected@file@percent }
\newlabel{tab:ch10_test_layers}{{10.1}{328}{Test layer selection determines coverage and feedback speed. Different layers (unit, integration, end-to-end, adversarial) validate different aspects: unit tests catch component regressions early; integration tests verify cross-component behavior; end-to-end tests validate user workflows; adversarial tests ensure security. Understanding what each layer validates helps teams design comprehensive test suites}{table.10.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Behavioral coverage beats ``average score'' optimization.}{328}{section*.82}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Test coverage matrix ensures comprehensive validation beyond aggregate scores. By mapping linguistic capabilities (vocabulary, negation, coreference) against test perturbation types (invariance, directional, minimum), teams can identify gaps in their test suites and ensure behavioral coverage across critical dimensions. This matrix approach prevents optimizing for average performance while missing edge cases.}}{329}{figure.10.2}\protected@file@percent }
\newlabel{fig:ch10_test_coverage_matrix}{{10.2}{329}{Test coverage matrix ensures comprehensive validation beyond aggregate scores. By mapping linguistic capabilities (vocabulary, negation, coreference) against test perturbation types (invariance, directional, minimum), teams can identify gaps in their test suites and ensure behavioral coverage across critical dimensions. This matrix approach prevents optimizing for average performance while missing edge cases}{figure.10.2}{}}
\newlabel{lst:ch10_unit_test_example}{{10.1}{330}{10.2.1 Unit Testing}{llmlisting.10.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces Unit test examples demonstrate how to validate individual LLM components in isolation. Each row shows a concrete test case for prompts, parsers, or retrieval functions, illustrating how unit tests catch regressions early and provide fast feedback during development. These examples guide teams in designing comprehensive unit test suites.}}{331}{table.10.2}\protected@file@percent }
\newlabel{tab:ch10_unit_test_examples}{{10.2}{331}{Unit test examples demonstrate how to validate individual LLM components in isolation. Each row shows a concrete test case for prompts, parsers, or retrieval functions, illustrating how unit tests catch regressions early and provide fast feedback during development. These examples guide teams in designing comprehensive unit test suites}{table.10.2}{}}
\newlabel{lst:ch10_integration_test_example}{{10.2}{332}{10.2.2 Integration Testing}{llmlisting.10.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.3}{\ignorespaces Integration test scenarios validate cross-component behavior in LLM systems. These scenarios (RAG chains, tool calling, agent handoffs) ensure that components work together correctly and catch interface mismatches, data flow errors, and emergent failures that unit tests miss. Understanding these scenarios helps teams design comprehensive integration test suites.}}{333}{table.10.3}\protected@file@percent }
\newlabel{tab:ch10_integration_test_scenarios}{{10.3}{333}{Integration test scenarios validate cross-component behavior in LLM systems. These scenarios (RAG chains, tool calling, agent handoffs) ensure that components work together correctly and catch interface mismatches, data flow errors, and emergent failures that unit tests miss. Understanding these scenarios helps teams design comprehensive integration test suites}{table.10.3}{}}
\newlabel{alg:ch10_e2e_test_execution}{{10.1}{334}{10.2.3 End-to-End Testing}{llmalgorithm.10.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.4}{\ignorespaces Adversarial test taxonomy organizes security testing for LLM systems. This taxonomy maps attack categories (prompt injection, jailbreaks, indirect injection) to detection methods and mitigation strategies, enabling systematic security validation aligned with OWASP LLM Top 10. Understanding this taxonomy helps teams design comprehensive security test suites.}}{336}{table.10.4}\protected@file@percent }
\newlabel{tab:ch10_adversarial_taxonomy}{{10.4}{336}{Adversarial test taxonomy organizes security testing for LLM systems. This taxonomy maps attack categories (prompt injection, jailbreaks, indirect injection) to detection methods and mitigation strategies, enabling systematic security validation aligned with OWASP LLM Top 10. Understanding this taxonomy helps teams design comprehensive security test suites}{table.10.4}{}}
\newlabel{lst:ch10_adversarial_test_suite}{{10.3}{336}{10.2.4 Adversarial Testing}{llmlisting.10.3}{}}
\@writefile{toc}{\contentsline {paragraph}{From exact oracles to property-based oracles.}{337}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Metamorphic testing for LLM workflows.}{337}{section*.89}\protected@file@percent }
\newlabel{alg:ch10_metamorphic_testing}{{10.2}{338}{Metamorphic testing for LLM workflows}{llmalgorithm.10.2}{}}
\newlabel{lst:ch10_metamorphic_py}{{10.4}{338}{Metamorphic testing for LLM workflows}{llmlisting.10.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Evaluation Metrics}{339}{section.10.4}\protected@file@percent }
\newlabel{sec:ch10-evaluation-metrics}{{10.4}{339}{Evaluation Metrics}{section.10.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Quantitative Metrics}{339}{subsection.10.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Qualitative Metrics}{339}{subsection.10.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Slice-based reporting is non-negotiable.}{339}{section*.91}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.5}{\ignorespaces Evaluation scorecard design determines release decisions and quality monitoring. A balanced scorecard covers multiple dimensions (correctness, faithfulness, safety, style, latency, cost), enabling teams to detect regressions across different quality aspects. In practice, teams should define a small set of \emph  {release gates} (hard thresholds) that block deployments, and a broader set of \emph  {monitoring metrics} (trend alerts) that signal gradual degradation.}}{340}{table.10.5}\protected@file@percent }
\newlabel{tab:ch10_scorecard}{{10.5}{340}{Evaluation scorecard design determines release decisions and quality monitoring. A balanced scorecard covers multiple dimensions (correctness, faithfulness, safety, style, latency, cost), enabling teams to detect regressions across different quality aspects. In practice, teams should define a small set of \emph {release gates} (hard thresholds) that block deployments, and a broader set of \emph {monitoring metrics} (trend alerts) that signal gradual degradation}{table.10.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Evaluation metrics dashboard provides operational visibility into system quality trends. Time-series visualizations track correctness, faithfulness, latency, and cost over time, enabling teams to detect regressions, validate improvements, and make data-driven release decisions. Such dashboards are essential for continuous quality monitoring and help teams identify trends before they become critical issues.}}{341}{figure.10.3}\protected@file@percent }
\newlabel{fig:ch10_metrics_dashboard}{{10.3}{341}{Evaluation metrics dashboard provides operational visibility into system quality trends. Time-series visualizations track correctness, faithfulness, latency, and cost over time, enabling teams to detect regressions, validate improvements, and make data-driven release decisions. Such dashboards are essential for continuous quality monitoring and help teams identify trends before they become critical issues}{figure.10.3}{}}
\newlabel{lst:ch10_metrics_aggregation}{{10.5}{341}{Slice-based reporting is non-negotiable}{llmlisting.10.5}{}}
\newlabel{alg:ch10_slice_based_reporting}{{10.3}{342}{Slice-based reporting is non-negotiable}{llmalgorithm.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Automated Evaluation Techniques}{344}{section.10.5}\protected@file@percent }
\newlabel{sec:ch10-automated-evaluation-techniques}{{10.5}{344}{Automated Evaluation Techniques}{section.10.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Golden Datasets}{344}{subsection.10.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}LLM-as-a-Judge}{344}{subsection.10.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Semantic Similarity Metrics}{344}{subsection.10.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Integrated evaluation pipeline combines multiple automated techniques for comprehensive quality assessment. Golden datasets provide reference-based validation, LLM-as-judge adds nuanced scoring, and semantic similarity captures meaning-level matches. This multi-technique approach reduces blind spots and provides robust quality signals, enabling teams to catch regressions across different quality dimensions.}}{345}{figure.10.4}\protected@file@percent }
\newlabel{fig:ch10_eval_pipeline_flow}{{10.4}{345}{Integrated evaluation pipeline combines multiple automated techniques for comprehensive quality assessment. Golden datasets provide reference-based validation, LLM-as-judge adds nuanced scoring, and semantic similarity captures meaning-level matches. This multi-technique approach reduces blind spots and provides robust quality signals, enabling teams to catch regressions across different quality dimensions}{figure.10.4}{}}
\newlabel{alg:ch10_golden_dataset_maintenance}{{10.4}{346}{10.4.1 Golden Datasets}{llmalgorithm.10.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Rubric scoring vs.\ pairwise preference.}{347}{section*.97}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Known judge biases and how to mitigate them.}{347}{section*.98}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Judge calibration curve maps automated scores to human-aligned values. The calibration mapping (e.g., isotonic regression) corrects for systematic biases (position, verbosity, self-enhancement), ensuring that judge scores reliably predict human judgments. Periodic recalibration prevents drift as judge behavior evolves, maintaining alignment between automated and human evaluation.}}{348}{figure.10.5}\protected@file@percent }
\newlabel{fig:ch10_judge_calibration_curve}{{10.5}{348}{Judge calibration curve maps automated scores to human-aligned values. The calibration mapping (e.g., isotonic regression) corrects for systematic biases (position, verbosity, self-enhancement), ensuring that judge scores reliably predict human judgments. Periodic recalibration prevents drift as judge behavior evolves, maintaining alignment between automated and human evaluation}{figure.10.5}{}}
\newlabel{alg:ch10_llm_judge_calibration}{{10.5}{348}{Known judge biases and how to mitigate them}{llmalgorithm.10.5}{}}
\newlabel{lst:ch10_judge_prompt}{{10.6}{349}{Known judge biases and how to mitigate them}{llmlisting.10.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Operational guidance.}{349}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Modern Evaluation Tooling and Standards}{350}{subsection.10.5.4}\protected@file@percent }
\newlabel{sec:ch10-modern-eval}{{10.5.4}{350}{Modern Evaluation Tooling and Standards}{subsection.10.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4.1}System-level eval harnesses.}{350}{subsubsection.10.5.4.1}\protected@file@percent }
\newlabel{lst:ch10_eval_harness_config}{{10.7}{351}{System-level eval harnesses}{llmlisting.10.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.6}{\ignorespaces Evaluation tool comparison helps teams select appropriate frameworks for their LLMOps pipelines. Different tools excel at different aspects (golden datasets, LLM-as-judge, RAG evaluation, CI/CD integration), and teams often combine multiple tools for comprehensive coverage. This comparison guides tool selection based on specific evaluation needs.}}{352}{table.10.6}\protected@file@percent }
\newlabel{tab:ch10_eval_tool_comparison}{{10.6}{352}{Evaluation tool comparison helps teams select appropriate frameworks for their LLMOps pipelines. Different tools excel at different aspects (golden datasets, LLM-as-judge, RAG evaluation, CI/CD integration), and teams often combine multiple tools for comprehensive coverage. This comparison guides tool selection based on specific evaluation needs}{table.10.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4.2}Benchmark taxonomies and multi-metric evaluation.}{352}{subsubsection.10.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4.3}RAG and evidence-grounded evaluation.}{352}{subsubsection.10.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.5.4.4}Security-oriented testing.}{352}{subsubsection.10.5.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.5}Statistical Treatment and Confidence}{353}{subsection.10.5.5}\protected@file@percent }
\newlabel{sec:ch10-statistical-confidence}{{10.5.5}{353}{Statistical Treatment and Confidence}{subsection.10.5.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Confidence intervals for noisy metrics.}{353}{section*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Paired comparisons beat unpaired comparisons.}{353}{section*.102}\protected@file@percent }
\newlabel{alg:ch10_bootstrap_gate}{{10.6}{353}{Paired comparisons beat unpaired comparisons}{llmalgorithm.10.6}{}}
\newlabel{lst:ch10_bootstrap_py}{{10.8}{354}{Paired comparisons beat unpaired comparisons}{llmlisting.10.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical note on non-determinism.}{354}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.6}Human-in-the-Loop Evaluation}{354}{section.10.6}\protected@file@percent }
\newlabel{sec:ch10-human-in-the-loop-evaluation}{{10.6}{354}{Human-in-the-Loop Evaluation}{section.10.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.7}{\ignorespaces Rubric design enables consistent human evaluation and LLM judge calibration. For evidence-grounded journalism assistance (Ishtar AI), ``hard fails'' correspond to non-negotiable constraints (faithfulness, safety violations) that block release; ``soft scores'' capture gradations of quality (correctness, attribution, neutrality, clarity) that guide improvements. This rubric structure converts subjective quality judgments into measurable, actionable signals.}}{357}{table.10.7}\protected@file@percent }
\newlabel{tab:ch10_ishtar_rubric}{{10.7}{357}{Rubric design enables consistent human evaluation and LLM judge calibration. For evidence-grounded journalism assistance (Ishtar AI), ``hard fails'' correspond to non-negotiable constraints (faithfulness, safety violations) that block release; ``soft scores'' capture gradations of quality (correctness, attribution, neutrality, clarity) that guide improvements. This rubric structure converts subjective quality judgments into measurable, actionable signals}{table.10.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.1}Rubrics, Reliability, and Adjudication}{357}{subsection.10.6.1}\protected@file@percent }
\newlabel{sec:ch10-hitl-rubrics}{{10.6.1}{357}{Rubrics, Reliability, and Adjudication}{subsection.10.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Adjudication and ``golden'' labels.}{358}{section*.113}\protected@file@percent }
\newlabel{alg:ch10_hitl_round}{{10.7}{358}{Adjudication and ``golden'' labels}{llmalgorithm.10.7}{}}
\newlabel{lst:ch10_annotation_schema}{{10.9}{358}{Adjudication and ``golden'' labels}{llmlisting.10.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Connecting human evaluation to automation.}{359}{section*.114}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.7}Robustness Testing}{359}{section.10.7}\protected@file@percent }
\newlabel{sec:ch10-robustness-testing}{{10.7}{359}{Robustness Testing}{section.10.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.1}Load Testing}{359}{subsection.10.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.2}Fault Injection}{359}{subsection.10.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.3}Prompt Injection Defense}{359}{subsection.10.7.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.8}{\ignorespaces Load test configurations define realistic stress scenarios for LLM systems. Different patterns (gradual ramp, spike, sustained load) reveal different failure modes, helping teams validate autoscaling policies, identify bottlenecks, and ensure systems meet latency SLOs under expected traffic. These configurations guide teams in designing comprehensive load testing strategies.}}{360}{table.10.8}\protected@file@percent }
\newlabel{tab:ch10_load_test_configs}{{10.8}{360}{Load test configurations define realistic stress scenarios for LLM systems. Different patterns (gradual ramp, spike, sustained load) reveal different failure modes, helping teams validate autoscaling policies, identify bottlenecks, and ensure systems meet latency SLOs under expected traffic. These configurations guide teams in designing comprehensive load testing strategies}{table.10.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.9}{\ignorespaces Fault injection matrix enables systematic chaos testing and resilience validation. For an LLM+RAG system, each row defines a reusable experiment with explicit success criteria, enabling teams to verify that failures degrade gracefully rather than catastrophically. This matrix structure converts ad hoc chaos testing into a repeatable, auditable resilience validation process.}}{362}{table.10.9}\protected@file@percent }
\newlabel{tab:ch10_fault_injection_matrix}{{10.9}{362}{Fault injection matrix enables systematic chaos testing and resilience validation. For an LLM+RAG system, each row defines a reusable experiment with explicit success criteria, enabling teams to verify that failures degrade gracefully rather than catastrophically. This matrix structure converts ad hoc chaos testing into a repeatable, auditable resilience validation process}{table.10.9}{}}
\@writefile{toc}{\contentsline {paragraph}{A minimal chaos experiment template.}{362}{section*.120}\protected@file@percent }
\newlabel{alg:ch10_fault_injection_experiment}{{10.8}{362}{A minimal chaos experiment template}{llmalgorithm.10.8}{}}
\newlabel{lst:ch10_load_test}{{10.10}{363}{A minimal chaos experiment template}{llmlisting.10.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.8}Regression Testing in CI/CD}{363}{section.10.8}\protected@file@percent }
\newlabel{sec:ch10-regression-testing-in-ci-cd}{{10.8}{363}{Regression Testing in CI/CD}{section.10.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces CI/CD evaluation gate flow integrates quality checks into deployment pipelines. Automated tests run on each change, metrics are compared against baselines, and gates block deployments when thresholds are violated. This ensures that regressions are caught before reaching production, maintaining quality while enabling rapid iteration. Failed gates trigger feedback loops that prevent broken changes from progressing.}}{364}{figure.10.6}\protected@file@percent }
\newlabel{fig:ch10_cicd_gate_flow}{{10.6}{364}{CI/CD evaluation gate flow integrates quality checks into deployment pipelines. Automated tests run on each change, metrics are compared against baselines, and gates block deployments when thresholds are violated. This ensures that regressions are caught before reaching production, maintaining quality while enabling rapid iteration. Failed gates trigger feedback loops that prevent broken changes from progressing}{figure.10.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Tiered evaluation gates integrate LLM evaluation into CI/CD. Fast gates protect developer velocity; full gates protect releases; canaries detect residual regressions in production.}}{366}{figure.10.7}\protected@file@percent }
\newlabel{fig:ch10_tiered_gates}{{10.7}{366}{Tiered evaluation gates integrate LLM evaluation into CI/CD. Fast gates protect developer velocity; full gates protect releases; canaries detect residual regressions in production}{figure.10.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.1}Tiered Evaluation Gates and Release Criteria}{366}{subsection.10.8.1}\protected@file@percent }
\newlabel{sec:ch10-tiered-gates}{{10.8.1}{366}{Tiered Evaluation Gates and Release Criteria}{subsection.10.8.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Connecting gates to statistical evidence.}{366}{section*.132}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.10}{\ignorespaces Tiered evaluation plan balances feedback speed with coverage depth. Different gate tiers (pre-merge smoke, nightly regression, pre-release full, canary) run at different cadences and cover different scopes, enabling fast developer feedback while ensuring comprehensive validation before release. ``Critical'' gates should be small, stable, and directly tied to known failure modes to maximize signal-to-noise ratio.}}{367}{table.10.10}\protected@file@percent }
\newlabel{tab:ch10_tiered_plan}{{10.10}{367}{Tiered evaluation plan balances feedback speed with coverage depth. Different gate tiers (pre-merge smoke, nightly regression, pre-release full, canary) run at different cadences and cover different scopes, enabling fast developer feedback while ensuring comprehensive validation before release. ``Critical'' gates should be small, stable, and directly tied to known failure modes to maximize signal-to-noise ratio}{table.10.10}{}}
\newlabel{lst:ch10_ci_eval}{{10.11}{367}{Connecting gates to statistical evidence}{llmlisting.10.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Resilience architecture ensures graceful failure handling through layered defenses. Fallback models activate when primary services fail; circuit breakers prevent cascading failures; timeouts prevent indefinite blocking; graceful degradation maintains partial functionality. This defense-in-depth approach maintains service continuity under adverse conditions, ensuring users receive responses even when components fail.}}{368}{figure.10.8}\protected@file@percent }
\newlabel{fig:ch10_resilience_architecture}{{10.8}{368}{Resilience architecture ensures graceful failure handling through layered defenses. Fallback models activate when primary services fail; circuit breakers prevent cascading failures; timeouts prevent indefinite blocking; graceful degradation maintains partial functionality. This defense-in-depth approach maintains service continuity under adverse conditions, ensuring users receive responses even when components fail}{figure.10.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.9}Resilience Strategies}{368}{section.10.9}\protected@file@percent }
\newlabel{sec:ch10-resilience-strategies}{{10.9}{368}{Resilience Strategies}{section.10.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.11}{\ignorespaces Failure mode taxonomy enables systematic resilience engineering. By mapping failure modes (provider outages, retrieval degradation, schema breakage, injection attacks, latency blow-ups) to detection signals and mitigations, teams can design proactive defenses rather than reactive fixes. This taxonomy converts ad hoc incident response into a structured resilience framework.}}{371}{table.10.11}\protected@file@percent }
\newlabel{tab:ch10_failure_modes}{{10.11}{371}{Failure mode taxonomy enables systematic resilience engineering. By mapping failure modes (provider outages, retrieval degradation, schema breakage, injection attacks, latency blow-ups) to detection signals and mitigations, teams can design proactive defenses rather than reactive fixes. This taxonomy converts ad hoc incident response into a structured resilience framework}{table.10.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.9.1}Failure Modes and Mitigations}{371}{subsection.10.9.1}\protected@file@percent }
\newlabel{sec:ch10-failure-modes-mitigations}{{10.9.1}{371}{Failure Modes and Mitigations}{subsection.10.9.1}{}}
\newlabel{lst:ch10_circuit_breaker}{{10.12}{371}{Failure Modes and Mitigations}{llmlisting.10.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Evaluation loop design enables continuous quality improvement. Production logs seed incident tests, converting failures into regression tests; curated datasets feed an offline evaluation runner for systematic validation; aggregated reports become CI/CD gates and dashboards, enabling data-driven release decisions. This closed-loop approach ensures that production observations drive quality improvements.}}{372}{figure.10.9}\protected@file@percent }
\newlabel{fig:ch10_ishtar_eval_loop}{{10.9}{372}{Evaluation loop design enables continuous quality improvement. Production logs seed incident tests, converting failures into regression tests; curated datasets feed an offline evaluation runner for systematic validation; aggregated reports become CI/CD gates and dashboards, enabling data-driven release decisions. This closed-loop approach ensures that production observations drive quality improvements}{figure.10.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.10}Case Study: Testing Ishtar AI}{372}{section.10.10}\protected@file@percent }
\newlabel{sec:ch10-case-study-testing-ishtar-ai}{{10.10}{372}{Case Study: Testing Ishtar AI}{section.10.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.10.1}Test Suite}{372}{subsection.10.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation architecture.}{372}{section*.142}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Slice design.}{372}{section*.143}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.10.2}Outcomes}{372}{subsection.10.10.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.12}{\ignorespaces Outcome metrics demonstrate systematic quality improvement through iterative evaluation. Ishtar AI's metrics show how prompt hardening and evidence-grounding reduce hallucination rates, while systematic evaluation detects performance regressions early. These outcomes validate that continuous evaluation converts production observations into measurable quality improvements.}}{373}{table.10.12}\protected@file@percent }
\newlabel{tab:ch10_ishtar_outcomes}{{10.12}{373}{Outcome metrics demonstrate systematic quality improvement through iterative evaluation. Ishtar AI's metrics show how prompt hardening and evidence-grounding reduce hallucination rates, while systematic evaluation detects performance regressions early. These outcomes validate that continuous evaluation converts production observations into measurable quality improvements}{table.10.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Representative impact on quality and operations.}{373}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lessons learned.}{373}{section*.145}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.11}Best Practices Checklist}{373}{section.10.11}\protected@file@percent }
\newlabel{sec:ch10-best-practices-checklist}{{10.11}{373}{Best Practices Checklist}{section.10.11}{}}
\@setckpt{ch10-testing-eval}{
\setcounter{page}{376}
\setcounter{equation}{0}
\setcounter{enumi}{8}
\setcounter{enumii}{4}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{4}
\setcounter{section}{11}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{12}
\setcounter{chapter}{10}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{Dfigchecks}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{32}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{1}
\setcounter{lstnumber}{10}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{73}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{22}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{1}
\setcounter{Item}{174}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
\setcounter{llmalgorithm}{8}
\setcounter{llmlisting}{12}
}
